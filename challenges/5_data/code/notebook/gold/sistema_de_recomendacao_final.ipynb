{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17423af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucas-\n",
      "[nltk_data]     nunes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lucas-\n",
      "[nltk_data]     nunes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lucas-\n",
      "[nltk_data]     nunes/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/lucas-nunes/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('rslp')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import glob\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"pt_core_news_sm\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer\n",
    "stemmer = RSLPStemmer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# BASE_PATH = '/home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/'\n",
    "BASE_PATH = '/home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7954f74",
   "metadata": {},
   "source": [
    "# 1. Carrega base consolidada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70abf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = f'{BASE_PATH}/talent_pool_sample.parquet'\n",
    "df = pd.read_parquet(df_path)\n",
    "# df = df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4d1b1",
   "metadata": {},
   "source": [
    "# 2. M√©todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ef8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_person_names(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove nomes de pessoas do texto usando reconhecimento de entidades nomeadas (NER).\n",
    "    \n",
    "    Par√¢metros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem entidades de tipo \"PER\" (pessoas).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if token.ent_type_ != \"PER\"])\n",
    "\n",
    "\n",
    "def normalize_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove acentos de um texto convertendo caracteres para ASCII.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem acentos.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove pontua√ß√£o de um texto, substituindo por espa√ßo.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem pontua√ß√£o.\n",
    "    \"\"\"\n",
    "    table = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def normalize_str(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto aplicando:\n",
    "      - Convers√£o para min√∫sculas\n",
    "      - Remo√ß√£o de n√∫meros\n",
    "      - Remo√ß√£o de pontua√ß√£o\n",
    "      - Remo√ß√£o de acentos\n",
    "      - Normaliza√ß√£o de espa√ßos\n",
    "    \n",
    "    Par√¢metros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto normalizado.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)           # remove n√∫meros\n",
    "    text = remove_punctuation(text)            # remove pontua√ß√£o\n",
    "    text = normalize_accents(text)             # remove acentos\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()   # normaliza espa√ßos\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokeniza texto em portugu√™s aplicando:\n",
    "      - Normaliza√ß√£o\n",
    "      - Remo√ß√£o de nomes de pessoas\n",
    "      - Tokeniza√ß√£o\n",
    "      - Remo√ß√£o de stopwords\n",
    "      - Stemiza√ß√£o\n",
    "    \n",
    "    Par√¢metros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de tokens processados.\n",
    "    \"\"\"\n",
    "    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)                                              # normaliza string\n",
    "        text = remove_person_names(text)                                        # remove nomes de pessoas\n",
    "        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza em portugu√™s\n",
    "        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]   # remove stopwords e tokens curtos\n",
    "        tokens = [stemmer.stem(t) for t in tokens]                              # aplica stemiza√ß√£o\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_fixed(df, campo_vetor, fitted_vectorizer, filename_prefix, batch_idx):\n",
    "    \"\"\"\n",
    "    Transforma um lote de dados usando um vetorizador TF-IDF j√° treinado e salva o resultado como Parquet.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        df (DataFrame): DataFrame contendo os dados de entrada.\n",
    "        campo_vetor (str): Nome da coluna com o texto a ser vetorizado.\n",
    "        fitted_vectorizer: Vetorizador TF-IDF j√° treinado.\n",
    "        filename_prefix (str): Prefixo para nome dos arquivos de sa√≠da.\n",
    "        batch_idx (int): √çndice do lote.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: DataFrame com os vetores TF-IDF.\n",
    "    \"\"\"\n",
    "    # Transforma o texto em matriz TF-IDF usando vocabul√°rio existente\n",
    "    vector_matrix = fitted_vectorizer.transform(df[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Cria DataFrame com os vetores\n",
    "    df_tfidf = pd.DataFrame(\n",
    "        vector_matrix.toarray(),\n",
    "        columns=fitted_vectorizer.get_feature_names_out(),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Salva lote\n",
    "    output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n",
    "    df_tfidf.to_parquet(output_file)\n",
    "    print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n",
    "\n",
    "    return df_tfidf\n",
    "\n",
    "\n",
    "def combine_vector_batches(batch_files, output_file):\n",
    "    \"\"\"\n",
    "    Combina todos os arquivos de lotes em um √∫nico DataFrame e salva como Parquet.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        batch_files (list): Lista de arquivos Parquet com vetores TF-IDF.\n",
    "        output_file (str): Nome do arquivo final combinado.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: DataFrame combinado com todos os vetores.\n",
    "    \"\"\"\n",
    "    print(\"Combinando todos os lotes...\")\n",
    "    combined_dfs = []\n",
    "\n",
    "    for i, file in enumerate(batch_files):\n",
    "        df_batch = pd.read_parquet(file) \n",
    "        combined_dfs.append(df_batch)\n",
    "        print(f\"Lote {i} carregado: {df_batch.shape}\")\n",
    "\n",
    "    # Combina todos os DataFrames\n",
    "    df_combined = pd.concat(combined_dfs, ignore_index=True)  # ignore_index=True reinicia os √≠ndices\n",
    "    df_combined.to_parquet(output_file)\n",
    "    print(f\"Conjunto combinado salvo: {df_combined.shape} -> {output_file}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=1000, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Treina o TF-IDF no dataset inteiro, processa em lotes e combina todos os lotes.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        df (DataFrame): DataFrame de entrada.\n",
    "        campo_vetor (str): Nome da coluna com os textos.\n",
    "        vectorizer: Vetorizador TF-IDF.\n",
    "        batch_size (int): Tamanho de cada lote (default=1000).\n",
    "        output_dir (str): Diret√≥rio de sa√≠da (default=\"output\").\n",
    "    \n",
    "    Retorna:\n",
    "        tuple: (DataFrame combinado, vetorizador treinado, lista de arquivos de lotes).\n",
    "    \"\"\"\n",
    "    # Treina o vetorizador em todos os dados\n",
    "    print(\"Treinando o vetorizador em todo o conjunto de dados...\")\n",
    "    vectorizer.fit(df[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    print(f\"Tamanho do vocabul√°rio: {len(vectorizer.vocabulary_)}\")\n",
    "    print(\"Exemplo de features:\", list(vectorizer.get_feature_names_out())[:10])\n",
    "\n",
    "    # Processa em lotes\n",
    "    print(\"Processando lotes com vocabul√°rio consistente...\")\n",
    "    filename_prefix = f\"{BASE_PATH}/application_processed_{campo_vetor}\"\n",
    "    batch_files = []\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "\n",
    "        # Vetoriza lote\n",
    "        #tokenize_and_vectorize_fixed(batch_df, campo_vetor, vectorizer, filename_prefix, batch_idx)\n",
    "        X_tfidf, df_tfidf = vetoriza_input(df, campo_vetor, vectorizer)\n",
    "\n",
    "        # Salva lote\n",
    "        output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n",
    "        df_tfidf.to_parquet(output_file)\n",
    "        print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n",
    "\n",
    "        batch_files.append(f\"{filename_prefix}_batch_{batch_idx}.parquet\")\n",
    "\n",
    "        print(f\"Lote {batch_idx} conclu√≠do (linhas {i} at√© {min(i+batch_size, len(df))})\")\n",
    "\n",
    "    print(f\"\\n{len(batch_files)} lotes processados com sucesso!\")\n",
    "    print(f\"Todos os lotes agora possuem as mesmas {len(vectorizer.vocabulary_)} features\")\n",
    "\n",
    "    # Combina todos os lotes\n",
    "    combined_output_file = f\"{BASE_PATH}/talent_pool_vectors_combined_{campo_vetor}.parquet\"\n",
    "    df_tfidf_combined = combine_vector_batches(batch_files, combined_output_file)\n",
    "\n",
    "    return df_tfidf_combined, vectorizer, batch_files\n",
    "\n",
    "\n",
    "def vetoriza_input(df_input, campo_vetor, vectorizer):\n",
    "    \"\"\"\n",
    "    Vetoriza um DataFrame usando TF-IDF e adiciona uma coluna com os vetores.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        df_input (DataFrame): DataFrame de entrada.\n",
    "        campo_vetor (str): Nome da coluna com os textos.\n",
    "        vectorizer: Vetorizador TF-IDF.\n",
    "    \n",
    "    Retorna:\n",
    "        tuple: (Matriz TF-IDF, DataFrame com coluna 'vetor_cv').\n",
    "    \"\"\"\n",
    "    # Treina o vetorizador nos dados de texto\n",
    "    vectorizer.fit(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Transforma o texto em vetores TF-IDF\n",
    "    X_tfidf = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Insere os vetores em uma coluna √∫nica no DataFrame\n",
    "    df_tfidf = df_input.copy()\n",
    "    df_tfidf['vetor_cv'] = [arr for arr in X_tfidf.toarray()]\n",
    "\n",
    "    return X_tfidf, df_tfidf\n",
    "\n",
    "\n",
    "def compute_similarity_batched(df_tfidf, campo_vetor, batch_size_sim=500, output_prefix='similarity_batch'):\n",
    "    \"\"\"\n",
    "    Calcula a similaridade do cosseno em lotes para lidar com grandes conjuntos de dados.\n",
    "    \n",
    "    Par√¢metros:\n",
    "        df_tfidf (DataFrame ou matriz): Dados vetorizados.\n",
    "        campo_vetor (str): Nome do campo usado para identifica√ß√£o.\n",
    "        batch_size_sim (int): Tamanho do lote para c√°lculo da similaridade (default=500).\n",
    "        output_prefix (str): Prefixo para os arquivos de sa√≠da (default='similarity_batch').\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de arquivos `.npy` contendo as matrizes de similaridade por lote.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "\n",
    "    n_samples = len(df_tfidf)\n",
    "    print(f\"Calculando similaridade para {n_samples} amostras em lotes de {batch_size_sim}\")\n",
    "\n",
    "    # Cria matriz de similaridade em lotes para gerenciar mem√≥ria\n",
    "    similarity_files = []\n",
    "\n",
    "    for i in range(0, n_samples, batch_size_sim):\n",
    "        batch_end = min(i + batch_size_sim, n_samples)\n",
    "        batch_data = df_tfidf.iloc[i:batch_end]\n",
    "\n",
    "        # Calcula similaridade entre este lote e TODOS os dados\n",
    "        batch_similarity = cosine_similarity(batch_data, df_tfidf)\n",
    "\n",
    "        # Salva similaridade do lote\n",
    "        batch_file = f'{BASE_PATH}/{output_prefix}_{i}_{batch_end}_{campo_vetor}.npy'\n",
    "        np.save(batch_file, batch_similarity)\n",
    "        similarity_files.append(batch_file)\n",
    "\n",
    "        print(f\"Similaridade do lote {i}-{batch_end} calculada: {batch_similarity.shape}\")\n",
    "\n",
    "    return similarity_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863ed21",
   "metadata": {},
   "source": [
    "# 3. Vetoriza√ß√£o e c√°lculo da similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd2c9233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetorizando campo: cv_pt\n",
      "Criando vocabul√°rio a partir de todos os dados...\n",
      "Treinando o vetorizador em todo o conjunto de dados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas-nunes/workspace/Postech/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabul√°rio: 7458\n",
      "Exemplo de features: ['abaix', 'abap', 'abastec', 'abbott', 'abc', 'abend', 'abert', 'abi', 'abil', 'abilit']\n",
      "Processando lotes com vocabul√°rio consistente...\n",
      "Lote 0 salvo com formato (300, 16) em /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/application_processed_cv_pt_batch_0.parquet\n",
      "Lote 0 conclu√≠do (linhas 0 at√© 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas-nunes/workspace/Postech/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 1 salvo com formato (300, 16) em /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/application_processed_cv_pt_batch_1.parquet\n",
      "Lote 1 conclu√≠do (linhas 100 at√© 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas-nunes/workspace/Postech/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 2 salvo com formato (300, 16) em /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/application_processed_cv_pt_batch_2.parquet\n",
      "Lote 2 conclu√≠do (linhas 200 at√© 300)\n",
      "\n",
      "3 lotes processados com sucesso!\n",
      "Todos os lotes agora possuem as mesmas 7458 features\n",
      "Combinando todos os lotes...\n",
      "Lote 0 carregado: (300, 16)\n",
      "Lote 1 carregado: (300, 16)\n",
      "Lote 2 carregado: (300, 16)\n",
      "Conjunto combinado salvo: (900, 16) -> /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/talent_pool_vectors_combined_cv_pt.parquet\n"
     ]
    }
   ],
   "source": [
    "lista_campos_vetor = ['cv_pt'] # trocar para os campos a serem utilizados\n",
    "\n",
    "for campo_vetor in lista_campos_vetor:\n",
    "  print(f\"Vetorizando campo: {campo_vetor}\")\n",
    "  print(\"Criando vocabul√°rio a partir de todos os dados...\")\n",
    "\n",
    "  # Ajusta o vetorizador em TODOS os dados para criar vocabul√°rio consistente\n",
    "  vectorizer = TfidfVectorizer(\n",
    "      tokenizer=tokenizer,\n",
    "      max_features=10000,\n",
    "      min_df=1,  # Allow terms that appear only once\n",
    "      max_df=0.95  # Only remove terms that appear in 95% of documents\n",
    "  )\n",
    "\n",
    "  # Processa e combina batches\n",
    "  df_talent_pool, vectorizer, batch_files = combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=100, output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aacbf",
   "metadata": {},
   "source": [
    "# 4. Tratamento para input inicial de dados do Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8894606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos o mesmo vetorizador que foi usado para os candidatos\n",
    "campo_vetor = 'cv_candidato'\n",
    "\n",
    "# DataFrame com os inputs do Streamlit\n",
    "cv_input = [\n",
    "'''\n",
    "    assistente administrativo\n",
    "santosbatista\n",
    "itapecerica da serra/sp\n",
    "29 anos ‚ñ™ brasileiro ‚ñ™ casado\n",
    "forma√ß√£o acad√™mica\n",
    "ÔÇ∑ bacharel - ci√™ncias cont√°beis\n",
    "centro universit√°rio √≠talo brasileiro\n",
    "jul/2015 - dez/2018\n",
    "ÔÇ∑ gradua√ß√£o - gest√£o financeira\n",
    "centro universit√°rio anhanguera\n",
    "jan/2013 - dez/2014\n",
    "habilidades\n",
    "ÔÇ∑ contas a pagar e receber\n",
    "ÔÇ∑ excel avan√ßado\n",
    "ÔÇ∑ indicadores kpi‚Äôs\n",
    "ÔÇ∑ notas fiscais, cfop‚Äôs\n",
    "ÔÇ∑ fechamento cont√°bil\n",
    "ÔÇ∑ emiss√£o de boletos\n",
    "ÔÇ∑ guias\n",
    "ÔÇ∑ impostos\n",
    "ÔÇ∑ budget\n",
    "ÔÇ∑ controladoria\n",
    "ÔÇ∑ sistemas integrados:\n",
    "totvs;\n",
    "folha matic;\n",
    "navision\n",
    "resumo profissional\n",
    "profissional com experi√™ncia nos departamentos financeiro,\n",
    "cont√°bil, fiscal e controladoria jur√≠dica. elabora√ß√£o e an√°lise de\n",
    "indicadores kpi‚Äôs de resultado, relat√≥rios, guias, gest√£o de\n",
    "pagamentos, notas fiscais, boletos, fechamento financeiro e\n",
    "cont√°bil fiscal.\n",
    "softwares erp protheus, folha matic, navision, elaw e sapiens,\n",
    "excel avan√ßado, (kpi's, pain√©is de dashboard e automatiza√ß√£o).\n",
    "hist√≥rico profissional\n",
    "ÔÇ∑ 01/2021 ‚Äì 07/2021 fcn contabilidade freight forwarder\n",
    "\n",
    "assistente cont√°bil\n",
    "concilia√ß√µes cont√°beis, financeira, folha de pagamento,\n",
    "fiscal, lan√ßamentos cont√°beis, exporta√ß√µes txt, an√°lise e\n",
    "elabora√ß√£o de relat√≥rios, fechamento cont√°bil, an√°lise\n",
    "fiscal e contabiliza√ß√£o de folha de pagamento, sistema\n",
    "folha matic.\n",
    "ÔÇ∑ 10/2020 ‚Äì 01/2021 almeida advogados\n",
    "assistente financeiro\n",
    "gest√£o de pagamentos, baixa de boletos, relat√≥rios gerenciais.\n",
    "ÔÇ∑ 04/2019 ‚Äì 06/2019 fedex brasil logistica e transporte ltda\n",
    "assistente juridico\n",
    "respons√°vel pelo fechamento mensal atrav√©s das\n",
    "apura√ß√µes de provis√µes e reclassifica√ß√µes cont√°beis,\n",
    "elabora√ß√£o de indicadores financeiros e desempenho,\n",
    "automatiza√ß√£o de planilhas, an√°lise de budget e real vs\n",
    "or√ßado.\n",
    "ÔÇ∑ 07/2017 ‚Äì 11/2018 atonanni constru√ß√µes e servi√ßos ltda\n",
    "assistente cont√°bil / fiscal\n",
    "lan√ßamento de notas fiscais, apura√ß√µes dos impostos (iss,\n",
    "pis, cofins, cprb, ir, csll).\n",
    "guias de pagamentos, sped fiscais, relat√≥rios, xml, cfop,\n",
    "ncm.\n",
    "ÔÇ∑ 06/2014 ‚Äì 07/2017 iss servisytem do brasil ltda\n",
    "assistente de controladoria\n",
    "contas a pagar e a receber, an√°lises cont√°beis e\n",
    "financeiras, reembolsos, p.o‚Äôs.\n",
    "gest√£o de custos, budget, real vs or√ßado, indicadores, kpi‚Äôs\n",
    "e mapeamento de melhorias.\n",
    "ÔÇ∑ 04/2013 ‚Äì 06/2014 n & n com√©rcio de alimentos ltda\n",
    "assistente financeiro\n",
    "contas a pagar e a receber, boletos, relat√≥rios gerenciais.\n",
    "baixa de notas fiscais, concila√ß√£o financeira, negocia√ß√µes\n",
    "de pagamentos\n",
    "'''\n",
    "]\n",
    "df_input = pd.DataFrame({'cv_candidato': cv_input})\n",
    "\n",
    "# Transformar usando o mesmo vetorizador (n√£o aplicar fit, apenas transform)\n",
    "X_tfidf_input = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "# Converter para formato de array e armazenar em um DataFrame\n",
    "df_input_job_desc = pd.DataFrame({\n",
    "    'vetor_cv': [X_tfidf_input.toarray()[0]]  # Armazena o vetor completo\n",
    "})\n",
    "df_input_job_desc = df_input_job_desc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e11ad3",
   "metadata": {},
   "source": [
    "# 5. Sistema de Recomenda√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e79315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando o Sistema de Recomenda√ß√£o de Talentos...\n",
      "Sistema de recomenda√ß√£o pronto!\n",
      "Foram carregados 900 perfis de candidatos\n",
      "Tamanho do vocabul√°rio: 7458 atributos\n"
     ]
    }
   ],
   "source": [
    "class TalentRecommendationSystem:\n",
    "    \"\"\"\n",
    "    Classe para recomenda√ß√£o de candidatos com base em similaridade de texto\n",
    "    utilizando vetores TF-IDF e similaridade do cosseno.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_tfidf, df_tfidf_input, vectorizer):\n",
    "        \"\"\"\n",
    "        Inicializa o sistema de recomenda√ß√£o.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        df_tfidf : pd.DataFrame\n",
    "            DataFrame contendo os vetores TF-IDF dos candidatos.\n",
    "        df_tfidf_input : pd.DataFrame\n",
    "            DataFrame contendo o vetor TF-IDF da descri√ß√£o de vaga.\n",
    "        vectorizer : TfidfVectorizer\n",
    "            Vetorizador usado para transformar os textos.\n",
    "        \"\"\"\n",
    "        self.df_tfidf = df_tfidf\n",
    "        self.df_tfidf_input = df_tfidf_input\n",
    "        self.vectorizer = vectorizer\n",
    "        self.similarity_cache = {}\n",
    "\n",
    "    def recommend_for_job_description(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Encontra os candidatos mais similares a uma descri√ß√£o de vaga.\n",
    "\n",
    "        Par√¢metros\n",
    "        ----------\n",
    "        top_n : int, opcional, default=10\n",
    "            N√∫mero de candidatos a retornar.\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        results : list of dict\n",
    "            Lista de dicion√°rios com informa√ß√µes dos candidatos recomendados.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        # Obt√©m o vetor da vaga e garante o formato correto\n",
    "        job_vector = self.df_tfidf_input['vetor_cv'].values[0]\n",
    "        if len(job_vector.shape) == 1:\n",
    "            job_vector = job_vector.reshape(1, -1)\n",
    "            \n",
    "        # Obt√©m os vetores dos candidatos e garante o formato correto\n",
    "        candidate_vectors = np.vstack([v for v in self.df_tfidf['vetor_cv'].values])\n",
    "        \n",
    "        # Garante que as dimens√µes sejam compat√≠veis\n",
    "        if job_vector.shape[1] != candidate_vectors.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"As dimens√µes dos vetores n√£o coincidem: \"\n",
    "                f\"vaga {job_vector.shape} vs candidatos {candidate_vectors.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Calcula a similaridade com todos os candidatos\n",
    "        similarities = cosine_similarity(job_vector, candidate_vectors)[0]\n",
    "\n",
    "        # Seleciona os melhores candidatos\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        top_scores = similarities[top_indices]\n",
    "\n",
    "        # Monta os resultados\n",
    "        results = []\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            candidate_info = {\n",
    "                'index': int(idx),\n",
    "                'match_score': float(score),\n",
    "                'nivel_profissional': self.df_tfidf.iloc[idx].get('nivel_profissional', 'N/A'),\n",
    "                'area_atuacao': self.df_tfidf.iloc[idx].get('area_atuacao', 'N/A'),\n",
    "                'nivel_academico': self.df_tfidf.iloc[idx].get('nivel_academico', 'N/A'),\n",
    "                'conhecimentos_preview': str(self.df_tfidf.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n",
    "            }\n",
    "            results.append(candidate_info)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Inicializa o sistema de recomenda√ß√£o\n",
    "print(\"Inicializando o Sistema de Recomenda√ß√£o de Talentos...\")\n",
    "talent_recommender = TalentRecommendationSystem(\n",
    "    df_talent_pool,\n",
    "    df_input_job_desc,\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"Sistema de recomenda√ß√£o pronto!\")\n",
    "print(f\"Foram carregados {len(df_talent_pool)} perfis de candidatos\")\n",
    "print(f\"Tamanho do vocabul√°rio: {len(vectorizer.vocabulary_)} atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0a31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_candidates = talent_recommender.recommend_for_job_description(top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a02a",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544dc176",
   "metadata": {},
   "source": [
    "### Talent pool with vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa9b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_talent_pool.to_parquet(f'{BASE_PATH}/talent_pool_final.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4716f",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9043b2c",
   "metadata": {},
   "source": [
    "### Alternative: Tokenizer-free vectorizer for easier deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ae66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment-ready vectorizer without custom tokenizer\n",
    "# This version uses standard preprocessing and is easier to load in production\n",
    "\n",
    "# Pre-process the text data using our custom tokenizer\n",
    "def preprocess_text_for_standard_vectorizer(text):\n",
    "    \"\"\"Apply our custom preprocessing but return as string for standard vectorizer\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        tokens = tokenizer(text)  # Use our custom tokenizer\n",
    "        return ' '.join(tokens) if tokens else ''\n",
    "    return ''\n",
    "\n",
    "# Apply preprocessing to the data\n",
    "print(\"Preprocessing text data...\")\n",
    "df_preprocessed = df.copy()\n",
    "df_preprocessed['cv_pt_processed'] = df_preprocessed['cv_pt'].apply(preprocess_text_for_standard_vectorizer)\n",
    "\n",
    "# Create a standard vectorizer (no custom tokenizer)\n",
    "standard_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    lowercase=False,  # Already handled in preprocessing\n",
    "    token_pattern=r'\\S+',  # Split on whitespace only\n",
    "    stop_words=None  # Already handled in preprocessing\n",
    ")\n",
    "\n",
    "# Train on preprocessed data\n",
    "print(\"Training standard vectorizer...\")\n",
    "standard_vectorizer.fit(df_preprocessed['cv_pt_processed'].fillna(\"\"))\n",
    "\n",
    "# Save the standard vectorizer (easier to load)\n",
    "standard_vectorizer_path = f'{BASE_PATH}/talent_vectorizer_standard.pkl'\n",
    "joblib.dump(standard_vectorizer, standard_vectorizer_path)\n",
    "print(f\"‚úÖ Standard vectorizer saved to: {standard_vectorizer_path}\")\n",
    "\n",
    "# Test loading\n",
    "test_standard = joblib.load(standard_vectorizer_path)\n",
    "print(f\"‚úÖ Standard vectorizer loaded successfully! Vocab size: {len(test_standard.vocabulary_)}\")\n",
    "\n",
    "# Also save the preprocessed data for consistency\n",
    "df_preprocessed.to_parquet(f'{BASE_PATH}/talent_pool_preprocessed.parquet')\n",
    "print(\"‚úÖ Preprocessed data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f63c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save vectorizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m joblib.dump(\u001b[43mvectorizer\u001b[49m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/talent_vectorizer.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVectorizer saved\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save vectorizer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Save to both locations for flexibility\n",
    "vectorizer_paths = [\n",
    "    f'{BASE_PATH}/talent_vectorizer.pkl',  # Data directory\n",
    "    '/home/lucas-nunes/workspace/Postech/challenges/5_data/code/streamlit/talent_vectorizer.pkl'  # Streamlit directory\n",
    "]\n",
    "\n",
    "for path in vectorizer_paths:\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save vectorizer\n",
    "        joblib.dump(vectorizer, path)\n",
    "        print(f\"‚úÖ Vectorizer saved to: {path}\")\n",
    "        \n",
    "        # Test loading immediately to verify it works\n",
    "        test_load = joblib.load(path)\n",
    "        print(f\"‚úÖ Vectorizer load test successful for: {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving vectorizer to {path}: {e}\")\n",
    "\n",
    "print(f\"\\nüìä Vectorizer Info:\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"   Max features: {vectorizer.max_features}\")\n",
    "print(f\"   Tokenizer: {vectorizer.tokenizer.__name__ if hasattr(vectorizer.tokenizer, '__name__') else 'Custom function'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ec5b59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Use relative path from BASE_PATH\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m vectorizer_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mBASE_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/talent_vectorizer.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading vectorizer from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorizer_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'BASE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Load vectorizer from .pkl (with proper tokenizer definition)\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# First, ensure tokenizer function is available for unpickling\n",
    "# The tokenizer function must be defined before loading the vectorizer\n",
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokeniza texto em portugu√™s aplicando:\n",
    "      - Normaliza√ß√£o\n",
    "      - Remo√ß√£o de nomes de pessoas\n",
    "      - Tokeniza√ß√£o\n",
    "      - Remo√ß√£o de stopwords\n",
    "      - Stemiza√ß√£o\n",
    "    \n",
    "    Par√¢metros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de tokens processados.\n",
    "    \"\"\"\n",
    "    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)                                              # normaliza string\n",
    "        text = remove_person_names(text)                                        # remove nomes de pessoas\n",
    "        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza em portugu√™s\n",
    "        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]   # remove stopwords e tokens curtos\n",
    "        tokens = [stemmer.stem(t) for t in tokens]                              # aplica stemiza√ß√£o\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "# Use relative path from BASE_PATH\n",
    "vectorizer_path = f'{BASE_PATH}/talent_vectorizer.pkl'\n",
    "print(f\"Loading vectorizer from: {vectorizer_path}\")\n",
    "\n",
    "try:\n",
    "    loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "    print(\"‚úÖ Vectorizer loaded successfully!\")\n",
    "    print(f\"Vocabulary size: {len(loaded_vectorizer.vocabulary_)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Vectorizer file not found at: {vectorizer_path}\")\n",
    "    print(\"Please run the training cells first to generate the vectorizer.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading vectorizer: {e}\")\n",
    "\n",
    "# talent_recommender = TalentRecommendationSystem(\n",
    "#     df_tfidf_combined,\n",
    "#     df_tfidf_input,\n",
    "#     loaded_vectorizer\n",
    "# )\n",
    "\n",
    "# matching_candidates = talent_recommender.recommend_for_job_description(top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ba9e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 635,\n",
       "  'match_score': 0.3765331245693815,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 35,\n",
       "  'match_score': 0.3765331245693815,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 335,\n",
       "  'match_score': 0.3765331245693815,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 642,\n",
       "  'match_score': 0.27760594374775704,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'P√≥s Gradua√ß√£o Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 42,\n",
       "  'match_score': 0.27760594374775704,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'P√≥s Gradua√ß√£o Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 342,\n",
       "  'match_score': 0.27760594374775704,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'P√≥s Gradua√ß√£o Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 593,\n",
       "  'match_score': 0.23116300821201136,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Cursando',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 893,\n",
       "  'match_score': 0.23116300821201136,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Cursando',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 293,\n",
       "  'match_score': 0.23116300821201136,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Cursando',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 288,\n",
       "  'match_score': 0.23036590948305294,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino M√©dio Completo',\n",
       "  'conhecimentos_preview': '...'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
