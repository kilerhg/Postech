{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17423af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/lucas-\n",
      "[nltk_data]     nunes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lucas-\n",
      "[nltk_data]     nunes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/lucas-\n",
      "[nltk_data]     nunes/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to /home/lucas-nunes/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('rslp')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import glob\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"pt_core_news_sm\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer\n",
    "stemmer = RSLPStemmer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# BASE_PATH = '/home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/'\n",
    "BASE_PATH = '/home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7954f74",
   "metadata": {},
   "source": [
    "# 1. Carrega base consolidada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70abf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = f'{BASE_PATH}/talent_pool_sample.parquet'\n",
    "df = pd.read_parquet(df_path)\n",
    "# df = df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4d1b1",
   "metadata": {},
   "source": [
    "# 2. Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2ef8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_person_names(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove nomes de pessoas do texto usando reconhecimento de entidades nomeadas (NER).\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem entidades de tipo \"PER\" (pessoas).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if token.ent_type_ != \"PER\"])\n",
    "\n",
    "\n",
    "def normalize_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove acentos de um texto convertendo caracteres para ASCII.\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem acentos.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove pontuação de um texto, substituindo por espaço.\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem pontuação.\n",
    "    \"\"\"\n",
    "    table = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def normalize_str(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto aplicando:\n",
    "      - Conversão para minúsculas\n",
    "      - Remoção de números\n",
    "      - Remoção de pontuação\n",
    "      - Remoção de acentos\n",
    "      - Normalização de espaços\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto normalizado.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)           # remove números\n",
    "    text = remove_punctuation(text)            # remove pontuação\n",
    "    text = normalize_accents(text)             # remove acentos\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()   # normaliza espaços\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokeniza texto em português aplicando:\n",
    "      - Normalização\n",
    "      - Remoção de nomes de pessoas\n",
    "      - Tokenização\n",
    "      - Remoção de stopwords\n",
    "      - Stemização\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de tokens processados.\n",
    "    \"\"\"\n",
    "    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)                                              # normaliza string\n",
    "        text = remove_person_names(text)                                        # remove nomes de pessoas\n",
    "        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza em português\n",
    "        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]   # remove stopwords e tokens curtos\n",
    "        tokens = [stemmer.stem(t) for t in tokens]                              # aplica stemização\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_fixed(df, campo_vetor, fitted_vectorizer, filename_prefix, batch_idx):\n",
    "    \"\"\"\n",
    "    Transforma um lote de dados usando um vetorizador TF-IDF já treinado e salva o resultado como Parquet.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame contendo os dados de entrada.\n",
    "        campo_vetor (str): Nome da coluna com o texto a ser vetorizado.\n",
    "        fitted_vectorizer: Vetorizador TF-IDF já treinado.\n",
    "        filename_prefix (str): Prefixo para nome dos arquivos de saída.\n",
    "        batch_idx (int): Índice do lote.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: DataFrame com os vetores TF-IDF.\n",
    "    \"\"\"\n",
    "    # Transforma o texto em matriz TF-IDF usando vocabulário existente\n",
    "    vector_matrix = fitted_vectorizer.transform(df[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Cria DataFrame com os vetores\n",
    "    df_tfidf = pd.DataFrame(\n",
    "        vector_matrix.toarray(),\n",
    "        columns=fitted_vectorizer.get_feature_names_out(),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Salva lote\n",
    "    output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n",
    "    df_tfidf.to_parquet(output_file)\n",
    "    print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n",
    "\n",
    "    return df_tfidf\n",
    "\n",
    "\n",
    "def combine_vector_batches(batch_files, output_file):\n",
    "    \"\"\"\n",
    "    Combina todos os arquivos de lotes em um único DataFrame e salva como Parquet.\n",
    "    \n",
    "    Parâmetros:\n",
    "        batch_files (list): Lista de arquivos Parquet com vetores TF-IDF.\n",
    "        output_file (str): Nome do arquivo final combinado.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: DataFrame combinado com todos os vetores.\n",
    "    \"\"\"\n",
    "    print(\"Combinando todos os lotes...\")\n",
    "    combined_dfs = []\n",
    "\n",
    "    for i, file in enumerate(batch_files):\n",
    "        df_batch = pd.read_parquet(file) \n",
    "        combined_dfs.append(df_batch)\n",
    "        print(f\"Lote {i} carregado: {df_batch.shape}\")\n",
    "\n",
    "    # Combina todos os DataFrames\n",
    "    df_combined = pd.concat(combined_dfs, ignore_index=True)  # ignore_index=True reinicia os índices\n",
    "    df_combined.to_parquet(output_file)\n",
    "    print(f\"Conjunto combinado salvo: {df_combined.shape} -> {output_file}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=1000, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Treina o TF-IDF no dataset inteiro, processa em lotes e combina todos os lotes.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame de entrada.\n",
    "        campo_vetor (str): Nome da coluna com os textos.\n",
    "        vectorizer: Vetorizador TF-IDF.\n",
    "        batch_size (int): Tamanho de cada lote (default=1000).\n",
    "        output_dir (str): Diretório de saída (default=\"output\").\n",
    "    \n",
    "    Retorna:\n",
    "        tuple: (DataFrame combinado, vetorizador treinado, lista de arquivos de lotes).\n",
    "    \"\"\"\n",
    "    # Treina o vetorizador em todos os dados\n",
    "    print(\"Treinando o vetorizador em todo o conjunto de dados...\")\n",
    "    vectorizer.fit(df[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    print(f\"Tamanho do vocabulário: {len(vectorizer.vocabulary_)}\")\n",
    "    print(\"Exemplo de features:\", list(vectorizer.get_feature_names_out())[:10])\n",
    "\n",
    "    # Processa em lotes\n",
    "    print(\"Processando lotes com vocabulário consistente...\")\n",
    "    filename_prefix = f\"{BASE_PATH}/application_processed_{campo_vetor}\"\n",
    "    batch_files = []\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "\n",
    "        # Vetoriza lote\n",
    "        #tokenize_and_vectorize_fixed(batch_df, campo_vetor, vectorizer, filename_prefix, batch_idx)\n",
    "        X_tfidf, df_tfidf = vetoriza_input(df, campo_vetor, vectorizer)\n",
    "\n",
    "        # Salva lote\n",
    "        output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n",
    "        df_tfidf.to_parquet(output_file)\n",
    "        print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n",
    "\n",
    "        batch_files.append(f\"{filename_prefix}_batch_{batch_idx}.parquet\")\n",
    "\n",
    "        print(f\"Lote {batch_idx} concluído (linhas {i} até {min(i+batch_size, len(df))})\")\n",
    "\n",
    "    print(f\"\\n{len(batch_files)} lotes processados com sucesso!\")\n",
    "    print(f\"Todos os lotes agora possuem as mesmas {len(vectorizer.vocabulary_)} features\")\n",
    "\n",
    "    # Combina todos os lotes\n",
    "    combined_output_file = f\"{BASE_PATH}/talent_pool_vectors_combined_{campo_vetor}.parquet\"\n",
    "    df_tfidf_combined = combine_vector_batches(batch_files, combined_output_file)\n",
    "\n",
    "    return df_tfidf_combined, vectorizer, batch_files\n",
    "\n",
    "\n",
    "def vetoriza_input(df_input, campo_vetor, vectorizer):\n",
    "    \"\"\"\n",
    "    Vetoriza um DataFrame usando TF-IDF e adiciona uma coluna com os vetores.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_input (DataFrame): DataFrame de entrada.\n",
    "        campo_vetor (str): Nome da coluna com os textos.\n",
    "        vectorizer: Vetorizador TF-IDF.\n",
    "    \n",
    "    Retorna:\n",
    "        tuple: (Matriz TF-IDF, DataFrame com coluna 'vetor_cv').\n",
    "    \"\"\"\n",
    "    # Treina o vetorizador nos dados de texto\n",
    "    vectorizer.fit(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Transforma o texto em vetores TF-IDF\n",
    "    X_tfidf = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Insere os vetores em uma coluna única no DataFrame\n",
    "    df_tfidf = df_input.copy()\n",
    "    df_tfidf['vetor_cv'] = [arr for arr in X_tfidf.toarray()]\n",
    "\n",
    "    return X_tfidf, df_tfidf\n",
    "\n",
    "\n",
    "def compute_similarity_batched(df_tfidf, campo_vetor, batch_size_sim=500, output_prefix='similarity_batch'):\n",
    "    \"\"\"\n",
    "    Calcula a similaridade do cosseno em lotes para lidar com grandes conjuntos de dados.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_tfidf (DataFrame ou matriz): Dados vetorizados.\n",
    "        campo_vetor (str): Nome do campo usado para identificação.\n",
    "        batch_size_sim (int): Tamanho do lote para cálculo da similaridade (default=500).\n",
    "        output_prefix (str): Prefixo para os arquivos de saída (default='similarity_batch').\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de arquivos `.npy` contendo as matrizes de similaridade por lote.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "\n",
    "    n_samples = len(df_tfidf)\n",
    "    print(f\"Calculando similaridade para {n_samples} amostras em lotes de {batch_size_sim}\")\n",
    "\n",
    "    # Cria matriz de similaridade em lotes para gerenciar memória\n",
    "    similarity_files = []\n",
    "\n",
    "    for i in range(0, n_samples, batch_size_sim):\n",
    "        batch_end = min(i + batch_size_sim, n_samples)\n",
    "        batch_data = df_tfidf.iloc[i:batch_end]\n",
    "\n",
    "        # Calcula similaridade entre este lote e TODOS os dados\n",
    "        batch_similarity = cosine_similarity(batch_data, df_tfidf)\n",
    "\n",
    "        # Salva similaridade do lote\n",
    "        batch_file = f'{BASE_PATH}/{output_prefix}_{i}_{batch_end}_{campo_vetor}.npy'\n",
    "        np.save(batch_file, batch_similarity)\n",
    "        similarity_files.append(batch_file)\n",
    "\n",
    "        print(f\"Similaridade do lote {i}-{batch_end} calculada: {batch_similarity.shape}\")\n",
    "\n",
    "    return similarity_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863ed21",
   "metadata": {},
   "source": [
    "# 3. Vetorização e cálculo da similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd2c9233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vetorizando campo: cv_pt\n",
      "Criando vocabulário a partir de todos os dados...\n",
      "Treinando o vetorizador em todo o conjunto de dados...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas-nunes/workspace/Postech/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho do vocabulário: 7458\n",
      "Exemplo de features: ['abaix', 'abap', 'abastec', 'abbott', 'abc', 'abend', 'abert', 'abi', 'abil', 'abilit']\n",
      "Processando lotes com vocabulário consistente...\n",
      "Lote 0 salvo com formato (300, 16) em /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/application_processed_cv_pt_batch_0.parquet\n",
      "Lote 0 concluído (linhas 0 até 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas-nunes/workspace/Postech/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 1 salvo com formato (300, 16) em /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/application_processed_cv_pt_batch_1.parquet\n",
      "Lote 1 concluído (linhas 100 até 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas-nunes/workspace/Postech/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote 2 salvo com formato (300, 16) em /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/application_processed_cv_pt_batch_2.parquet\n",
      "Lote 2 concluído (linhas 200 até 300)\n",
      "\n",
      "3 lotes processados com sucesso!\n",
      "Todos os lotes agora possuem as mesmas 7458 features\n",
      "Combinando todos os lotes...\n",
      "Lote 0 carregado: (300, 16)\n",
      "Lote 1 carregado: (300, 16)\n",
      "Lote 2 carregado: (300, 16)\n",
      "Conjunto combinado salvo: (900, 16) -> /home/lucas-nunes/workspace/Postech/challenges/5_data/data/gold/talent_pool_vectors_combined_cv_pt.parquet\n"
     ]
    }
   ],
   "source": [
    "lista_campos_vetor = ['cv_pt'] # trocar para os campos a serem utilizados\n",
    "\n",
    "for campo_vetor in lista_campos_vetor:\n",
    "  print(f\"Vetorizando campo: {campo_vetor}\")\n",
    "  print(\"Criando vocabulário a partir de todos os dados...\")\n",
    "\n",
    "  # Ajusta o vetorizador em TODOS os dados para criar vocabulário consistente\n",
    "  vectorizer = TfidfVectorizer(\n",
    "      tokenizer=tokenizer,\n",
    "      max_features=10000,\n",
    "      min_df=1,  # Allow terms that appear only once\n",
    "      max_df=0.95  # Only remove terms that appear in 95% of documents\n",
    "  )\n",
    "\n",
    "  # Processa e combina batches\n",
    "  df_talent_pool, vectorizer, batch_files = combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=100, output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aacbf",
   "metadata": {},
   "source": [
    "# 4. Tratamento para input inicial de dados do Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8894606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos o mesmo vetorizador que foi usado para os candidatos\n",
    "campo_vetor = 'cv_candidato'\n",
    "\n",
    "# DataFrame com os inputs do Streamlit\n",
    "cv_input = [\n",
    "'''\n",
    "    assistente administrativo\n",
    "santosbatista\n",
    "itapecerica da serra/sp\n",
    "29 anos ▪ brasileiro ▪ casado\n",
    "formação acadêmica\n",
    " bacharel - ciências contábeis\n",
    "centro universitário ítalo brasileiro\n",
    "jul/2015 - dez/2018\n",
    " graduação - gestão financeira\n",
    "centro universitário anhanguera\n",
    "jan/2013 - dez/2014\n",
    "habilidades\n",
    " contas a pagar e receber\n",
    " excel avançado\n",
    " indicadores kpi’s\n",
    " notas fiscais, cfop’s\n",
    " fechamento contábil\n",
    " emissão de boletos\n",
    " guias\n",
    " impostos\n",
    " budget\n",
    " controladoria\n",
    " sistemas integrados:\n",
    "totvs;\n",
    "folha matic;\n",
    "navision\n",
    "resumo profissional\n",
    "profissional com experiência nos departamentos financeiro,\n",
    "contábil, fiscal e controladoria jurídica. elaboração e análise de\n",
    "indicadores kpi’s de resultado, relatórios, guias, gestão de\n",
    "pagamentos, notas fiscais, boletos, fechamento financeiro e\n",
    "contábil fiscal.\n",
    "softwares erp protheus, folha matic, navision, elaw e sapiens,\n",
    "excel avançado, (kpi's, painéis de dashboard e automatização).\n",
    "histórico profissional\n",
    " 01/2021 – 07/2021 fcn contabilidade freight forwarder\n",
    "\n",
    "assistente contábil\n",
    "conciliações contábeis, financeira, folha de pagamento,\n",
    "fiscal, lançamentos contábeis, exportações txt, análise e\n",
    "elaboração de relatórios, fechamento contábil, análise\n",
    "fiscal e contabilização de folha de pagamento, sistema\n",
    "folha matic.\n",
    " 10/2020 – 01/2021 almeida advogados\n",
    "assistente financeiro\n",
    "gestão de pagamentos, baixa de boletos, relatórios gerenciais.\n",
    " 04/2019 – 06/2019 fedex brasil logistica e transporte ltda\n",
    "assistente juridico\n",
    "responsável pelo fechamento mensal através das\n",
    "apurações de provisões e reclassificações contábeis,\n",
    "elaboração de indicadores financeiros e desempenho,\n",
    "automatização de planilhas, análise de budget e real vs\n",
    "orçado.\n",
    " 07/2017 – 11/2018 atonanni construções e serviços ltda\n",
    "assistente contábil / fiscal\n",
    "lançamento de notas fiscais, apurações dos impostos (iss,\n",
    "pis, cofins, cprb, ir, csll).\n",
    "guias de pagamentos, sped fiscais, relatórios, xml, cfop,\n",
    "ncm.\n",
    " 06/2014 – 07/2017 iss servisytem do brasil ltda\n",
    "assistente de controladoria\n",
    "contas a pagar e a receber, análises contábeis e\n",
    "financeiras, reembolsos, p.o’s.\n",
    "gestão de custos, budget, real vs orçado, indicadores, kpi’s\n",
    "e mapeamento de melhorias.\n",
    " 04/2013 – 06/2014 n & n comércio de alimentos ltda\n",
    "assistente financeiro\n",
    "contas a pagar e a receber, boletos, relatórios gerenciais.\n",
    "baixa de notas fiscais, concilação financeira, negociações\n",
    "de pagamentos\n",
    "'''\n",
    "]\n",
    "df_input = pd.DataFrame({'cv_candidato': cv_input})\n",
    "\n",
    "# Transformar usando o mesmo vetorizador (não aplicar fit, apenas transform)\n",
    "X_tfidf_input = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "# Converter para formato de array e armazenar em um DataFrame\n",
    "df_input_job_desc = pd.DataFrame({\n",
    "    'vetor_cv': [X_tfidf_input.toarray()[0]]  # Armazena o vetor completo\n",
    "})\n",
    "df_input_job_desc = df_input_job_desc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e11ad3",
   "metadata": {},
   "source": [
    "# 5. Sistema de Recomendação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e79315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando o Sistema de Recomendação de Talentos...\n",
      "Sistema de recomendação pronto!\n",
      "Foram carregados 900 perfis de candidatos\n",
      "Tamanho do vocabulário: 7458 atributos\n"
     ]
    }
   ],
   "source": [
    "class TalentRecommendationSystem:\n",
    "    \"\"\"\n",
    "    Classe para recomendação de candidatos com base em similaridade de texto\n",
    "    utilizando vetores TF-IDF e similaridade do cosseno.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_tfidf, df_tfidf_input, vectorizer):\n",
    "        \"\"\"\n",
    "        Inicializa o sistema de recomendação.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        df_tfidf : pd.DataFrame\n",
    "            DataFrame contendo os vetores TF-IDF dos candidatos.\n",
    "        df_tfidf_input : pd.DataFrame\n",
    "            DataFrame contendo o vetor TF-IDF da descrição de vaga.\n",
    "        vectorizer : TfidfVectorizer\n",
    "            Vetorizador usado para transformar os textos.\n",
    "        \"\"\"\n",
    "        self.df_tfidf = df_tfidf\n",
    "        self.df_tfidf_input = df_tfidf_input\n",
    "        self.vectorizer = vectorizer\n",
    "        self.similarity_cache = {}\n",
    "\n",
    "    def recommend_for_job_description(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Encontra os candidatos mais similares a uma descrição de vaga.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        top_n : int, opcional, default=10\n",
    "            Número de candidatos a retornar.\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        results : list of dict\n",
    "            Lista de dicionários com informações dos candidatos recomendados.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        # Obtém o vetor da vaga e garante o formato correto\n",
    "        job_vector = self.df_tfidf_input['vetor_cv'].values[0]\n",
    "        if len(job_vector.shape) == 1:\n",
    "            job_vector = job_vector.reshape(1, -1)\n",
    "            \n",
    "        # Obtém os vetores dos candidatos e garante o formato correto\n",
    "        candidate_vectors = np.vstack([v for v in self.df_tfidf['vetor_cv'].values])\n",
    "        \n",
    "        # Garante que as dimensões sejam compatíveis\n",
    "        if job_vector.shape[1] != candidate_vectors.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"As dimensões dos vetores não coincidem: \"\n",
    "                f\"vaga {job_vector.shape} vs candidatos {candidate_vectors.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Calcula a similaridade com todos os candidatos\n",
    "        similarities = cosine_similarity(job_vector, candidate_vectors)[0]\n",
    "\n",
    "        # Seleciona os melhores candidatos\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        top_scores = similarities[top_indices]\n",
    "\n",
    "        # Monta os resultados\n",
    "        results = []\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            candidate_info = {\n",
    "                'index': int(idx),\n",
    "                'match_score': float(score),\n",
    "                'nivel_profissional': self.df_tfidf.iloc[idx].get('nivel_profissional', 'N/A'),\n",
    "                'area_atuacao': self.df_tfidf.iloc[idx].get('area_atuacao', 'N/A'),\n",
    "                'nivel_academico': self.df_tfidf.iloc[idx].get('nivel_academico', 'N/A'),\n",
    "                'conhecimentos_preview': str(self.df_tfidf.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n",
    "            }\n",
    "            results.append(candidate_info)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Inicializa o sistema de recomendação\n",
    "print(\"Inicializando o Sistema de Recomendação de Talentos...\")\n",
    "talent_recommender = TalentRecommendationSystem(\n",
    "    df_talent_pool,\n",
    "    df_input_job_desc,\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"Sistema de recomendação pronto!\")\n",
    "print(f\"Foram carregados {len(df_talent_pool)} perfis de candidatos\")\n",
    "print(f\"Tamanho do vocabulário: {len(vectorizer.vocabulary_)} atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0a31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_candidates = talent_recommender.recommend_for_job_description(top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d296a02a",
   "metadata": {},
   "source": [
    "## Write"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544dc176",
   "metadata": {},
   "source": [
    "### Talent pool with vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa9b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_talent_pool.to_parquet(f'{BASE_PATH}/talent_pool_final.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4716f",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9043b2c",
   "metadata": {},
   "source": [
    "### Alternative: Tokenizer-free vectorizer for easier deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ae66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a deployment-ready vectorizer without custom tokenizer\n",
    "# This version uses standard preprocessing and is easier to load in production\n",
    "\n",
    "# Pre-process the text data using our custom tokenizer\n",
    "def preprocess_text_for_standard_vectorizer(text):\n",
    "    \"\"\"Apply our custom preprocessing but return as string for standard vectorizer\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        tokens = tokenizer(text)  # Use our custom tokenizer\n",
    "        return ' '.join(tokens) if tokens else ''\n",
    "    return ''\n",
    "\n",
    "# Apply preprocessing to the data\n",
    "print(\"Preprocessing text data...\")\n",
    "df_preprocessed = df.copy()\n",
    "df_preprocessed['cv_pt_processed'] = df_preprocessed['cv_pt'].apply(preprocess_text_for_standard_vectorizer)\n",
    "\n",
    "# Create a standard vectorizer (no custom tokenizer)\n",
    "standard_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    min_df=1,\n",
    "    max_df=0.95,\n",
    "    lowercase=False,  # Already handled in preprocessing\n",
    "    token_pattern=r'\\S+',  # Split on whitespace only\n",
    "    stop_words=None  # Already handled in preprocessing\n",
    ")\n",
    "\n",
    "# Train on preprocessed data\n",
    "print(\"Training standard vectorizer...\")\n",
    "standard_vectorizer.fit(df_preprocessed['cv_pt_processed'].fillna(\"\"))\n",
    "\n",
    "# Save the standard vectorizer (easier to load)\n",
    "standard_vectorizer_path = f'{BASE_PATH}/talent_vectorizer_standard.pkl'\n",
    "joblib.dump(standard_vectorizer, standard_vectorizer_path)\n",
    "print(f\"✅ Standard vectorizer saved to: {standard_vectorizer_path}\")\n",
    "\n",
    "# Test loading\n",
    "test_standard = joblib.load(standard_vectorizer_path)\n",
    "print(f\"✅ Standard vectorizer loaded successfully! Vocab size: {len(test_standard.vocabulary_)}\")\n",
    "\n",
    "# Also save the preprocessed data for consistency\n",
    "df_preprocessed.to_parquet(f'{BASE_PATH}/talent_pool_preprocessed.parquet')\n",
    "print(\"✅ Preprocessed data saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f63c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Save vectorizer\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m joblib.dump(\u001b[43mvectorizer\u001b[49m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/talent_vectorizer.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVectorizer saved\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Save vectorizer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Save to both locations for flexibility\n",
    "vectorizer_paths = [\n",
    "    f'{BASE_PATH}/talent_vectorizer.pkl',  # Data directory\n",
    "    '/home/lucas-nunes/workspace/Postech/challenges/5_data/code/streamlit/talent_vectorizer.pkl'  # Streamlit directory\n",
    "]\n",
    "\n",
    "for path in vectorizer_paths:\n",
    "    try:\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save vectorizer\n",
    "        joblib.dump(vectorizer, path)\n",
    "        print(f\"✅ Vectorizer saved to: {path}\")\n",
    "        \n",
    "        # Test loading immediately to verify it works\n",
    "        test_load = joblib.load(path)\n",
    "        print(f\"✅ Vectorizer load test successful for: {path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving vectorizer to {path}: {e}\")\n",
    "\n",
    "print(f\"\\n📊 Vectorizer Info:\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "print(f\"   Max features: {vectorizer.max_features}\")\n",
    "print(f\"   Tokenizer: {vectorizer.tokenizer.__name__ if hasattr(vectorizer.tokenizer, '__name__') else 'Custom function'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9ec5b59",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BASE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Use relative path from BASE_PATH\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m vectorizer_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mBASE_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/talent_vectorizer.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading vectorizer from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorizer_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'BASE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Load vectorizer from .pkl (with proper tokenizer definition)\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# First, ensure tokenizer function is available for unpickling\n",
    "# The tokenizer function must be defined before loading the vectorizer\n",
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokeniza texto em português aplicando:\n",
    "      - Normalização\n",
    "      - Remoção de nomes de pessoas\n",
    "      - Tokenização\n",
    "      - Remoção de stopwords\n",
    "      - Stemização\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de tokens processados.\n",
    "    \"\"\"\n",
    "    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)                                              # normaliza string\n",
    "        text = remove_person_names(text)                                        # remove nomes de pessoas\n",
    "        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza em português\n",
    "        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]   # remove stopwords e tokens curtos\n",
    "        tokens = [stemmer.stem(t) for t in tokens]                              # aplica stemização\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "# Use relative path from BASE_PATH\n",
    "vectorizer_path = f'{BASE_PATH}/talent_vectorizer.pkl'\n",
    "print(f\"Loading vectorizer from: {vectorizer_path}\")\n",
    "\n",
    "try:\n",
    "    loaded_vectorizer = joblib.load(vectorizer_path)\n",
    "    print(\"✅ Vectorizer loaded successfully!\")\n",
    "    print(f\"Vocabulary size: {len(loaded_vectorizer.vocabulary_)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Vectorizer file not found at: {vectorizer_path}\")\n",
    "    print(\"Please run the training cells first to generate the vectorizer.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading vectorizer: {e}\")\n",
    "\n",
    "# talent_recommender = TalentRecommendationSystem(\n",
    "#     df_tfidf_combined,\n",
    "#     df_tfidf_input,\n",
    "#     loaded_vectorizer\n",
    "# )\n",
    "\n",
    "# matching_candidates = talent_recommender.recommend_for_job_description(top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ba9e22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 635,\n",
       "  'match_score': 0.3765331245693815,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 35,\n",
       "  'match_score': 0.3765331245693815,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 335,\n",
       "  'match_score': 0.3765331245693815,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 642,\n",
       "  'match_score': 0.27760594374775704,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Pós Graduação Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 42,\n",
       "  'match_score': 0.27760594374775704,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Pós Graduação Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 342,\n",
       "  'match_score': 0.27760594374775704,\n",
       "  'nivel_profissional': 'Analista',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Pós Graduação Completo',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 593,\n",
       "  'match_score': 0.23116300821201136,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Cursando',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 893,\n",
       "  'match_score': 0.23116300821201136,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Cursando',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 293,\n",
       "  'match_score': 0.23116300821201136,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Superior Cursando',\n",
       "  'conhecimentos_preview': '...'},\n",
       " {'index': 288,\n",
       "  'match_score': 0.23036590948305294,\n",
       "  'nivel_profissional': '',\n",
       "  'area_atuacao': 'N/A',\n",
       "  'nivel_academico': 'Ensino Médio Completo',\n",
       "  'conhecimentos_preview': '...'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_candidates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
