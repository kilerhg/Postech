{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17423af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('rslp')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import unicodedata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import glob\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"pt_core_news_sm\")\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import RSLPStemmer\n",
    "stemmer = RSLPStemmer()\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7954f74",
   "metadata": {},
   "source": [
    "# 1. Carrega base consolidada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70abf756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_path = '/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/Dados/silver/dados_processed/talent_pool.parquet'\n",
    "df = pd.read_parquet(df_path)\n",
    "df = df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb4d1b1",
   "metadata": {},
   "source": [
    "# 2. Métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ef8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_person_names(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove nomes de pessoas do texto usando reconhecimento de entidades nomeadas (NER).\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem entidades de tipo \"PER\" (pessoas).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.text for token in doc if token.ent_type_ != \"PER\"])\n",
    "\n",
    "\n",
    "def normalize_accents(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove acentos de um texto convertendo caracteres para ASCII.\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem acentos.\n",
    "    \"\"\"\n",
    "    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove pontuação de um texto, substituindo por espaço.\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto sem pontuação.\n",
    "    \"\"\"\n",
    "    table = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "    return text.translate(table)\n",
    "\n",
    "\n",
    "def normalize_str(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normaliza texto aplicando:\n",
    "      - Conversão para minúsculas\n",
    "      - Remoção de números\n",
    "      - Remoção de pontuação\n",
    "      - Remoção de acentos\n",
    "      - Normalização de espaços\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        str: Texto normalizado.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \" \", text)           # remove números\n",
    "    text = remove_punctuation(text)            # remove pontuação\n",
    "    text = normalize_accents(text)             # remove acentos\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()   # normaliza espaços\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenizer(text: str):\n",
    "    \"\"\"\n",
    "    Tokeniza texto em português aplicando:\n",
    "      - Normalização\n",
    "      - Remoção de nomes de pessoas\n",
    "      - Tokenização\n",
    "      - Remoção de stopwords\n",
    "      - Stemização\n",
    "    \n",
    "    Parâmetros:\n",
    "        text (str): Texto de entrada.\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de tokens processados.\n",
    "    \"\"\"\n",
    "    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n",
    "    if isinstance(text, str):\n",
    "        text = normalize_str(text)                                              # normaliza string\n",
    "        text = remove_person_names(text)                                        # remove nomes de pessoas\n",
    "        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza em português\n",
    "        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]   # remove stopwords e tokens curtos\n",
    "        tokens = [stemmer.stem(t) for t in tokens]                              # aplica stemização\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "\n",
    "def tokenize_and_vectorize_fixed(df, campo_vetor, fitted_vectorizer, filename_prefix, batch_idx):\n",
    "    \"\"\"\n",
    "    Transforma um lote de dados usando um vetorizador TF-IDF já treinado e salva o resultado como Parquet.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame contendo os dados de entrada.\n",
    "        campo_vetor (str): Nome da coluna com o texto a ser vetorizado.\n",
    "        fitted_vectorizer: Vetorizador TF-IDF já treinado.\n",
    "        filename_prefix (str): Prefixo para nome dos arquivos de saída.\n",
    "        batch_idx (int): Índice do lote.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: DataFrame com os vetores TF-IDF.\n",
    "    \"\"\"\n",
    "    # Transforma o texto em matriz TF-IDF usando vocabulário existente\n",
    "    vector_matrix = fitted_vectorizer.transform(df[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Cria DataFrame com os vetores\n",
    "    df_tfidf = pd.DataFrame(\n",
    "        vector_matrix.toarray(),\n",
    "        columns=fitted_vectorizer.get_feature_names_out(),\n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # Salva lote\n",
    "    output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n",
    "    df_tfidf.to_parquet(output_file)\n",
    "    print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n",
    "\n",
    "    return df_tfidf\n",
    "\n",
    "\n",
    "def combine_vector_batches(batch_files, output_file):\n",
    "    \"\"\"\n",
    "    Combina todos os arquivos de lotes em um único DataFrame e salva como Parquet.\n",
    "    \n",
    "    Parâmetros:\n",
    "        batch_files (list): Lista de arquivos Parquet com vetores TF-IDF.\n",
    "        output_file (str): Nome do arquivo final combinado.\n",
    "    \n",
    "    Retorna:\n",
    "        DataFrame: DataFrame combinado com todos os vetores.\n",
    "    \"\"\"\n",
    "    print(\"Combinando todos os lotes...\")\n",
    "    combined_dfs = []\n",
    "\n",
    "    for i, file in enumerate(batch_files):\n",
    "        df_batch = pd.read_parquet(file)  # ✅ Lê o arquivo Parquet\n",
    "        combined_dfs.append(df_batch)\n",
    "        print(f\"Lote {i} carregado: {df_batch.shape}\")\n",
    "\n",
    "    # Combina todos os DataFrames\n",
    "    df_combined = pd.concat(combined_dfs, ignore_index=True)  # ignore_index=True reinicia os índices\n",
    "    df_combined.to_parquet(output_file)\n",
    "    print(f\"Conjunto combinado salvo: {df_combined.shape} -> {output_file}\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "\n",
    "def combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=1000, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    Treina o TF-IDF no dataset inteiro, processa em lotes e combina todos os lotes.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (DataFrame): DataFrame de entrada.\n",
    "        campo_vetor (str): Nome da coluna com os textos.\n",
    "        vectorizer: Vetorizador TF-IDF.\n",
    "        batch_size (int): Tamanho de cada lote (default=1000).\n",
    "        output_dir (str): Diretório de saída (default=\"output\").\n",
    "    \n",
    "    Retorna:\n",
    "        tuple: (DataFrame combinado, vetorizador treinado, lista de arquivos de lotes).\n",
    "    \"\"\"\n",
    "    # Treina o vetorizador em todos os dados\n",
    "    print(\"Treinando o vetorizador em todo o conjunto de dados...\")\n",
    "    vectorizer.fit(df[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    print(f\"Tamanho do vocabulário: {len(vectorizer.vocabulary_)}\")\n",
    "    print(\"Exemplo de features:\", list(vectorizer.get_feature_names_out())[:10])\n",
    "\n",
    "    # Processa em lotes\n",
    "    print(\"Processando lotes com vocabulário consistente...\")\n",
    "    filename_prefix = f\"/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/application_processed_{campo_vetor}\"\n",
    "    batch_files = []\n",
    "\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batch_idx = i // batch_size\n",
    "\n",
    "        # Vetoriza lote\n",
    "        #tokenize_and_vectorize_fixed(batch_df, campo_vetor, vectorizer, filename_prefix, batch_idx)\n",
    "        X_tfidf, df_tfidf = vetoriza_input(df, campo_vetor, vectorizer)\n",
    "\n",
    "        # Salva lote\n",
    "        output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n",
    "        df_tfidf.to_parquet(output_file)\n",
    "        print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n",
    "\n",
    "        batch_files.append(f\"{filename_prefix}_batch_{batch_idx}.parquet\")\n",
    "\n",
    "        print(f\"Lote {batch_idx} concluído (linhas {i} até {min(i+batch_size, len(df))})\")\n",
    "\n",
    "    print(f\"\\n{len(batch_files)} lotes processados com sucesso!\")\n",
    "    print(f\"Todos os lotes agora possuem as mesmas {len(vectorizer.vocabulary_)} features\")\n",
    "\n",
    "    # Combina todos os lotes\n",
    "    combined_output_file = f\"/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_pool_vectors_combined_{campo_vetor}.parquet\"\n",
    "    df_tfidf_combined = combine_vector_batches(batch_files, combined_output_file)\n",
    "\n",
    "    return df_tfidf_combined, vectorizer, batch_files\n",
    "\n",
    "\n",
    "def vetoriza_input(df_input, campo_vetor, vectorizer):\n",
    "    \"\"\"\n",
    "    Vetoriza um DataFrame usando TF-IDF e adiciona uma coluna com os vetores.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_input (DataFrame): DataFrame de entrada.\n",
    "        campo_vetor (str): Nome da coluna com os textos.\n",
    "        vectorizer: Vetorizador TF-IDF.\n",
    "    \n",
    "    Retorna:\n",
    "        tuple: (Matriz TF-IDF, DataFrame com coluna 'vetor_cv').\n",
    "    \"\"\"\n",
    "    # Treina o vetorizador nos dados de texto\n",
    "    vectorizer.fit(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Transforma o texto em vetores TF-IDF\n",
    "    X_tfidf = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "    # Insere os vetores em uma coluna única no DataFrame\n",
    "    df_tfidf = df_input.copy()\n",
    "    df_tfidf['vetor_cv'] = [arr for arr in X_tfidf.toarray()]\n",
    "\n",
    "    return X_tfidf, df_tfidf\n",
    "\n",
    "\n",
    "def compute_similarity_batched(df_tfidf, campo_vetor, batch_size_sim=500, output_prefix='similarity_batch'):\n",
    "    \"\"\"\n",
    "    Calcula a similaridade do cosseno em lotes para lidar com grandes conjuntos de dados.\n",
    "    \n",
    "    Parâmetros:\n",
    "        df_tfidf (DataFrame ou matriz): Dados vetorizados.\n",
    "        campo_vetor (str): Nome do campo usado para identificação.\n",
    "        batch_size_sim (int): Tamanho do lote para cálculo da similaridade (default=500).\n",
    "        output_prefix (str): Prefixo para os arquivos de saída (default='similarity_batch').\n",
    "    \n",
    "    Retorna:\n",
    "        list: Lista de arquivos `.npy` contendo as matrizes de similaridade por lote.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "\n",
    "    n_samples = len(df_tfidf)\n",
    "    print(f\"Calculando similaridade para {n_samples} amostras em lotes de {batch_size_sim}\")\n",
    "\n",
    "    # Cria matriz de similaridade em lotes para gerenciar memória\n",
    "    similarity_files = []\n",
    "\n",
    "    for i in range(0, n_samples, batch_size_sim):\n",
    "        batch_end = min(i + batch_size_sim, n_samples)\n",
    "        batch_data = df_tfidf.iloc[i:batch_end]\n",
    "\n",
    "        # Calcula similaridade entre este lote e TODOS os dados\n",
    "        batch_similarity = cosine_similarity(batch_data, df_tfidf)\n",
    "\n",
    "        # Salva similaridade do lote\n",
    "        batch_file = f'/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/{output_prefix}_{i}_{batch_end}_{campo_vetor}.npy'\n",
    "        np.save(batch_file, batch_similarity)\n",
    "        similarity_files.append(batch_file)\n",
    "\n",
    "        print(f\"Similaridade do lote {i}-{batch_end} calculada: {batch_similarity.shape}\")\n",
    "\n",
    "    return similarity_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9863ed21",
   "metadata": {},
   "source": [
    "# 3. Vetorização e cálculo da similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c9233",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_campos_vetor = ['cv_pt'] # trocar para os campos a serem utilizados\n",
    "\n",
    "for campo_vetor in lista_campos_vetor:\n",
    "  print(f\"Vetorizando campo: {campo_vetor}\")\n",
    "  print(\"Criando vocabulário a partir de todos os dados...\")\n",
    "\n",
    "  # Ajusta o vetorizador em TODOS os dados para criar vocabulário consistente\n",
    "  vectorizer = TfidfVectorizer(\n",
    "      tokenizer=tokenizer,\n",
    "      max_features=10000,\n",
    "      min_df=1,  # Allow terms that appear only once\n",
    "      max_df=0.95  # Only remove terms that appear in 95% of documents\n",
    "  )\n",
    "\n",
    "  # Processa e combina batches\n",
    "  df_tfidf_combined, vectorizer, batch_files = combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=100, output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30aacbf",
   "metadata": {},
   "source": [
    "# 4. Tratamento para input inicial de dados do Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8894606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usaremos o mesmo vetorizador que foi usado para os candidatos\n",
    "campo_vetor = 'cv_candidato'\n",
    "\n",
    "# DataFrame com os inputs do Streamlit\n",
    "cv_input = [\n",
    "'''\n",
    "    assistente administrativo\n",
    "santosbatista\n",
    "itapecerica da serra/sp\n",
    "29 anos ▪ brasileiro ▪ casado\n",
    "formação acadêmica\n",
    " bacharel - ciências contábeis\n",
    "centro universitário ítalo brasileiro\n",
    "jul/2015 - dez/2018\n",
    " graduação - gestão financeira\n",
    "centro universitário anhanguera\n",
    "jan/2013 - dez/2014\n",
    "habilidades\n",
    " contas a pagar e receber\n",
    " excel avançado\n",
    " indicadores kpi’s\n",
    " notas fiscais, cfop’s\n",
    " fechamento contábil\n",
    " emissão de boletos\n",
    " guias\n",
    " impostos\n",
    " budget\n",
    " controladoria\n",
    " sistemas integrados:\n",
    "totvs;\n",
    "folha matic;\n",
    "navision\n",
    "resumo profissional\n",
    "profissional com experiência nos departamentos financeiro,\n",
    "contábil, fiscal e controladoria jurídica. elaboração e análise de\n",
    "indicadores kpi’s de resultado, relatórios, guias, gestão de\n",
    "pagamentos, notas fiscais, boletos, fechamento financeiro e\n",
    "contábil fiscal.\n",
    "softwares erp protheus, folha matic, navision, elaw e sapiens,\n",
    "excel avançado, (kpi's, painéis de dashboard e automatização).\n",
    "histórico profissional\n",
    " 01/2021 – 07/2021 fcn contabilidade freight forwarder\n",
    "\n",
    "assistente contábil\n",
    "conciliações contábeis, financeira, folha de pagamento,\n",
    "fiscal, lançamentos contábeis, exportações txt, análise e\n",
    "elaboração de relatórios, fechamento contábil, análise\n",
    "fiscal e contabilização de folha de pagamento, sistema\n",
    "folha matic.\n",
    " 10/2020 – 01/2021 almeida advogados\n",
    "assistente financeiro\n",
    "gestão de pagamentos, baixa de boletos, relatórios gerenciais.\n",
    " 04/2019 – 06/2019 fedex brasil logistica e transporte ltda\n",
    "assistente juridico\n",
    "responsável pelo fechamento mensal através das\n",
    "apurações de provisões e reclassificações contábeis,\n",
    "elaboração de indicadores financeiros e desempenho,\n",
    "automatização de planilhas, análise de budget e real vs\n",
    "orçado.\n",
    " 07/2017 – 11/2018 atonanni construções e serviços ltda\n",
    "assistente contábil / fiscal\n",
    "lançamento de notas fiscais, apurações dos impostos (iss,\n",
    "pis, cofins, cprb, ir, csll).\n",
    "guias de pagamentos, sped fiscais, relatórios, xml, cfop,\n",
    "ncm.\n",
    " 06/2014 – 07/2017 iss servisytem do brasil ltda\n",
    "assistente de controladoria\n",
    "contas a pagar e a receber, análises contábeis e\n",
    "financeiras, reembolsos, p.o’s.\n",
    "gestão de custos, budget, real vs orçado, indicadores, kpi’s\n",
    "e mapeamento de melhorias.\n",
    " 04/2013 – 06/2014 n & n comércio de alimentos ltda\n",
    "assistente financeiro\n",
    "contas a pagar e a receber, boletos, relatórios gerenciais.\n",
    "baixa de notas fiscais, concilação financeira, negociações\n",
    "de pagamentos\n",
    "'''\n",
    "]\n",
    "df_input = pd.DataFrame({'cv_candidato': cv_input})\n",
    "\n",
    "# Transformar usando o mesmo vetorizador (não aplicar fit, apenas transform)\n",
    "X_tfidf_input = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n",
    "\n",
    "# Converter para formato de array e armazenar em um DataFrame\n",
    "df_tfidf_input = pd.DataFrame({\n",
    "    'vetor_cv': [X_tfidf_input.toarray()[0]]  # Armazena o vetor completo\n",
    "})\n",
    "df_tfidf_input = df_tfidf_input.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e11ad3",
   "metadata": {},
   "source": [
    "# 5. Sistema de Recomendação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e79315",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TalentRecommendationSystem:\n",
    "    \"\"\"\n",
    "    Classe para recomendação de candidatos com base em similaridade de texto\n",
    "    utilizando vetores TF-IDF e similaridade do cosseno.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_tfidf, df_tfidf_input, vectorizer):\n",
    "        \"\"\"\n",
    "        Inicializa o sistema de recomendação.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        df_tfidf : pd.DataFrame\n",
    "            DataFrame contendo os vetores TF-IDF dos candidatos.\n",
    "        df_tfidf_input : pd.DataFrame\n",
    "            DataFrame contendo o vetor TF-IDF da descrição de vaga.\n",
    "        vectorizer : TfidfVectorizer\n",
    "            Vetorizador usado para transformar os textos.\n",
    "        \"\"\"\n",
    "        self.df_tfidf = df_tfidf\n",
    "        self.df_tfidf_input = df_tfidf_input\n",
    "        self.vectorizer = vectorizer\n",
    "        self.similarity_cache = {}\n",
    "\n",
    "    def recommend_for_job_description(self, top_n=10):\n",
    "        \"\"\"\n",
    "        Encontra os candidatos mais similares a uma descrição de vaga.\n",
    "\n",
    "        Parâmetros\n",
    "        ----------\n",
    "        top_n : int, opcional, default=10\n",
    "            Número de candidatos a retornar.\n",
    "\n",
    "        Retorno\n",
    "        -------\n",
    "        results : list of dict\n",
    "            Lista de dicionários com informações dos candidatos recomendados.\n",
    "        \"\"\"\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        \n",
    "        # Obtém o vetor da vaga e garante o formato correto\n",
    "        job_vector = self.df_tfidf_input['vetor_cv'].values[0]\n",
    "        if len(job_vector.shape) == 1:\n",
    "            job_vector = job_vector.reshape(1, -1)\n",
    "            \n",
    "        # Obtém os vetores dos candidatos e garante o formato correto\n",
    "        candidate_vectors = np.vstack([v for v in self.df_tfidf['vetor_cv'].values])\n",
    "        \n",
    "        # Garante que as dimensões sejam compatíveis\n",
    "        if job_vector.shape[1] != candidate_vectors.shape[1]:\n",
    "            raise ValueError(\n",
    "                f\"As dimensões dos vetores não coincidem: \"\n",
    "                f\"vaga {job_vector.shape} vs candidatos {candidate_vectors.shape}\"\n",
    "            )\n",
    "        \n",
    "        # Calcula a similaridade com todos os candidatos\n",
    "        similarities = cosine_similarity(job_vector, candidate_vectors)[0]\n",
    "\n",
    "        # Seleciona os melhores candidatos\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_n]\n",
    "        top_scores = similarities[top_indices]\n",
    "\n",
    "        # Monta os resultados\n",
    "        results = []\n",
    "        for idx, score in zip(top_indices, top_scores):\n",
    "            candidate_info = {\n",
    "                'index': int(idx),\n",
    "                'match_score': float(score),\n",
    "                'nivel_profissional': self.df_tfidf.iloc[idx].get('nivel_profissional', 'N/A'),\n",
    "                'area_atuacao': self.df_tfidf.iloc[idx].get('area_atuacao', 'N/A'),\n",
    "                'nivel_academico': self.df_tfidf.iloc[idx].get('nivel_academico', 'N/A'),\n",
    "                'conhecimentos_preview': str(self.df_tfidf.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n",
    "            }\n",
    "            results.append(candidate_info)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Inicializa o sistema de recomendação\n",
    "print(\"Inicializando o Sistema de Recomendação de Talentos...\")\n",
    "talent_recommender = TalentRecommendationSystem(\n",
    "    df_tfidf_combined,\n",
    "    df_tfidf_input,\n",
    "    vectorizer\n",
    ")\n",
    "\n",
    "print(\"Sistema de recomendação pronto!\")\n",
    "print(f\"Foram carregados {len(df_tfidf_combined)} perfis de candidatos\")\n",
    "print(f\"Tamanho do vocabulário: {len(vectorizer.vocabulary_)} atributos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_candidates = talent_recommender.recommend_for_job_description(top_n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "posdatascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
