{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38481,"status":"ok","timestamp":1759066098540,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"RiqvGiFzF8sY","outputId":"4ec06e8d-73a8-4ee9-9308-7cb6421f2578"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Unzipping stemmers/rslp.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","nltk.download(\"punkt_tab\")\n","nltk.download('rslp')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import random\n","import time\n","import string\n","import unicodedata\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","import multiprocessing\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","import glob\n","import spacy.cli\n","spacy.cli.download(\"pt_core_news_sm\")\n","import spacy\n","nlp = spacy.load(\"pt_core_news_sm\")\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import RSLPStemmer\n","stemmer = RSLPStemmer()\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJxi1mELXasP","executionInfo":{"status":"ok","timestamp":1759066324639,"user_tz":180,"elapsed":18600,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"}},"outputId":"c0e653f9-a547-4cff-84d0-8ecc4000f42a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"-zjvi5nDq2tn"},"source":["# 1. Carrega base consolidada"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":14861,"status":"ok","timestamp":1759066343738,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"FQRb3J0Vz-U0"},"outputs":[],"source":["df_path = '/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/Dados/silver/dados_processed/application_processed.parquet'\n","df = pd.read_parquet('/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/Dados/silver/dados_processed/application_processed.parquet')\n","df = df.head(1000)"]},{"cell_type":"markdown","metadata":{"id":"bAoQicT0rqU-"},"source":["# 2. Métodos"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":33,"status":"ok","timestamp":1759066343773,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"PTxj_M1S20BL"},"outputs":[],"source":["def remove_person_names(text: str) -> str:\n","    doc = nlp(text)\n","    return \" \".join([token.text for token in doc if token.ent_type_ != \"PER\"])\n","\n","def normalize_accents(text: str) -> str:\n","    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n","\n","def remove_punctuation(text: str) -> str:\n","    table = str.maketrans({key: \" \" for key in string.punctuation})\n","    return text.translate(table)\n","\n","def normalize_str(text: str) -> str:\n","    text = text.lower()\n","    text = re.sub(r\"\\d+\", \" \", text)           # remove números\n","    text = remove_punctuation(text)            # remove pontuação\n","    text = normalize_accents(text)             # remove acentos\n","    text = re.sub(r\"\\s+\", \" \", text).strip()   # normaliza espaços\n","    return text\n","\n","def tokenizer(text: str):\n","    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n","    if isinstance(text, str):\n","        text = normalize_str(text)                                              # normaliza string\n","        text = remove_person_names(text)                                        # remove nomes\n","        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza para a lingua portuguesa\n","        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]\n","        tokens = [stemmer.stem(t) for t in tokens]                              # stemiza tokens\n","        return tokens\n","    return None\n","\n","def tokenize_and_vectorize_fixed(df, fitted_vectorizer, filename_prefix, batch_idx):\n","    # Transformar (não fit_transform) para usar o vocabulário existente\n","    vector_matrix = fitted_vectorizer.transform(df[campo_vetor].fillna(\"\"))\n","\n","    # Converter para DataFrame com nomes de colunas consistentes\n","    df_tfidf = pd.DataFrame(\n","        vector_matrix.toarray(),\n","        columns=fitted_vectorizer.get_feature_names_out(),\n","        index=df.index  # Preservar os índices originais\n","    )\n","\n","    # Salvar lote\n","    output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n","    df_tfidf.to_parquet(output_file)\n","    print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n","\n","    return df_tfidf\n","\n","def combine_vector_batches(batch_files, output_file):\n","    \"\"\"Combinar todos os arquivos de lote em um único arquivo de forma eficiente\"\"\"\n","    print(\"Combinando todos os lotes...\")\n","\n","    combined_dfs = []\n","    for i, file in enumerate(batch_files):\n","        df_batch = pd.read_parquet(file)\n","        combined_dfs.append(df_batch)\n","        print(f\"Lote {i} carregado: {df_batch.shape}\")\n","\n","    # Combinar todos os lotes\n","    df_combined = pd.concat(combined_dfs, ignore_index=False)  # Mantém os índices originais\n","\n","    # Salvar resultado combinado\n","    df_combined.to_parquet(output_file)\n","    print(f\"Conjunto combinado salvo: {df_combined.shape} -> {output_file}\")\n","\n","    return df_combined\n","\n","def compute_similarity_batched(df_tfidf, campo_vetor, batch_size_sim=500, output_prefix='similarity_batch'):\n","    \"\"\"Calcular similaridade do cosseno em lotes para lidar com grandes conjuntos de dados\"\"\"\n","    from sklearn.metrics.pairwise import cosine_similarity\n","    import numpy as np\n","\n","    n_samples = len(df_tfidf)\n","    print(f\"Calculando similaridade para {n_samples} amostras em lotes de {batch_size_sim}\")\n","\n","    # Criar matriz de similaridade em lotes para gerenciar memória\n","    similarity_files = []\n","\n","    for i in range(0, n_samples, batch_size_sim):\n","        batch_end = min(i + batch_size_sim, n_samples)\n","        batch_data = df_tfidf.iloc[i:batch_end]\n","\n","        # Calcular similaridade entre este lote e TODOS os dados\n","        batch_similarity = cosine_similarity(batch_data, df_tfidf)\n","\n","        # Salvar similaridade do lote\n","        batch_file = f'/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/{output_prefix}_{i}_{batch_end}_{campo_vetor}.npy'\n","        np.save(batch_file, batch_similarity)\n","        similarity_files.append(batch_file)\n","\n","        print(f\"Similaridade do lote {i}-{batch_end} calculada: {batch_similarity.shape}\")\n","\n","    return similarity_files"]},{"cell_type":"markdown","source":["# 3. Vetorização e cálculo da similaridade"],"metadata":{"id":"b9dyyPn2lh7k"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":219340,"status":"ok","timestamp":1759066563115,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"T4T-SRgu7L_n","outputId":"c4a83864-5ceb-45c2-8662-884cffe0585c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vetorizando campo: cv_pt_cleaned\n","Criando vocabulário a partir de todos os dados...\n","Treinando o vetorizador em todo o conjunto de dados...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Tamanho do vocabulário: 7578\n","Exemplo de features: ['...', 'aaa', 'aba', 'abac', 'abaix', 'abandon', 'abap', 'abastec', 'abb', 'abbott']\n","Processando lotes com vocabulário consistente...\n","Lote 0 salvo com formato (1000, 7578) em /content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/application_processed_cv_pt_cleaned.parquet_batch_0.parquet\n","Lote 0 concluído (linhas 0 até 1000)\n","\n","1 lotes processados com sucesso!\n","Todos os lotes agora possuem as mesmas 7578 features\n","Combinando todos os lotes...\n","Lote 0 carregado: (1000, 7578)\n","Conjunto combinado salvo: (1000, 7578) -> /content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_pool_vectors_combined_cv_pt_cleaned.parquet\n","Formato final da matriz TF-IDF: (1000, 7578)\n","Calculando similaridade do cosseno em lotes...\n","Calculando similaridade para 1000 amostras em lotes de 500\n","Similaridade do lote 0-500 calculada: (500, 1000)\n","Similaridade do lote 500-1000 calculada: (500, 1000)\n","Vetorizando campo: endereco\n","Criando vocabulário a partir de todos os dados...\n","Treinando o vetorizador em todo o conjunto de dados...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Tamanho do vocabulário: 18\n","Exemplo de features: ['bah', 'catarin', 'ce', 'distrit', 'feder', 'geral', 'grand', 'gross', 'jan', 'mat']\n","Processando lotes com vocabulário consistente...\n","Lote 0 salvo com formato (1000, 18) em /content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/application_processed_endereco.parquet_batch_0.parquet\n","Lote 0 concluído (linhas 0 até 1000)\n","\n","1 lotes processados com sucesso!\n","Todos os lotes agora possuem as mesmas 18 features\n","Combinando todos os lotes...\n","Lote 0 carregado: (1000, 18)\n","Conjunto combinado salvo: (1000, 18) -> /content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_pool_vectors_combined_endereco.parquet\n","Formato final da matriz TF-IDF: (1000, 18)\n","Calculando similaridade do cosseno em lotes...\n","Calculando similaridade para 1000 amostras em lotes de 500\n","Similaridade do lote 0-500 calculada: (500, 1000)\n","Similaridade do lote 500-1000 calculada: (500, 1000)\n"]}],"source":["lista_campos_vetor = ['cv_pt_cleaned', 'endereco'] # trocar para os campos a serem utilizados\n","\n","for campo_vetor in lista_campos_vetor:\n","  print(f\"Vetorizando campo: {campo_vetor}\")\n","  print(\"Criando vocabulário a partir de todos os dados...\")\n","  # Ajusta o vetorizador em TODOS os dados para criar vocabulário consistente\n","  vectorizer = TfidfVectorizer(\n","      tokenizer=tokenizer,\n","      max_features=10000,  # Limitar o tamanho do vocabulário para controlar memória\n","      min_df=2,            # Ignorar termos que aparecem em menos de 2 documentos\n","      max_df=0.8           # Ignorar termos que aparecem em mais de 80% dos documentos\n","  )\n","\n","  # Ajusta em todos os dados para criar o vocabulário\n","  print(\"Treinando o vetorizador em todo o conjunto de dados...\")\n","  vectorizer.fit(df[campo_vetor].fillna(\"\"))\n","\n","  print(f\"Tamanho do vocabulário: {len(vectorizer.vocabulary_)}\")\n","  print(\"Exemplo de features:\", list(vectorizer.get_feature_names_out())[:10])\n","\n","  # Processar em lotes com vocabulário consistente\n","  print(\"Processando lotes com vocabulário consistente...\")\n","  batch_size = 1000\n","  filename_prefix = f'/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/application_processed_{campo_vetor}.parquet'\n","\n","  batch_files = []\n","  for i in range(0, len(df), batch_size):\n","      batch_df = df.iloc[i:i+batch_size]\n","      batch_idx = i // batch_size\n","\n","      # Processar lote com vocabulário consistente\n","      tokenize_and_vectorize_fixed(batch_df, vectorizer, filename_prefix, batch_idx)\n","      batch_files.append(f\"{filename_prefix}_batch_{batch_idx}.parquet\")\n","\n","      print(f'Lote {batch_idx} concluído (linhas {i} até {min(i+batch_size, len(df))})')\n","\n","  print(f\"\\n{len(batch_files)} lotes processados com sucesso!\")\n","  print(f\"Todos os lotes agora possuem as mesmas {len(vectorizer.vocabulary_)} features\")\n","\n","  # Combinar todos os lotes\n","  combined_output_file = f'/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_pool_vectors_combined_{campo_vetor}.parquet'\n","  df_tfidf_combined = combine_vector_batches(batch_files, combined_output_file)\n","\n","  print(f\"Formato final da matriz TF-IDF: {df_tfidf_combined.shape}\")\n","\n","  # Calcular similaridade em lotes controláveis\n","  print(\"Calculando similaridade do cosseno em lotes...\")\n","  similarity_files = compute_similarity_batched(df_tfidf_combined, campo_vetor, batch_size_sim=500)"]},{"cell_type":"markdown","source":["# 4. Cálculo da similaridade"],"metadata":{"id":"_0KOOzr6qV5k"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBq4BS4-WoP8","executionInfo":{"status":"ok","timestamp":1758982325955,"user_tz":180,"elapsed":4,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"}},"outputId":"966eb7b9-3bc2-4da0-f179-1a7a8485916d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Talent Recommendation System...\n","✅ Recommendation system ready!\n","Loaded 1000 candidate profiles\n","Vocabulary size: 7578 features\n"]}],"source":["# Step 5: Efficient Recommendation System\n","class TalentRecommendationSystem:\n","    def __init__(self, df_tfidf, df_application_original, vectorizer):\n","        self.df_tfidf = df_tfidf\n","        self.df_application = df_application_original\n","        self.vectorizer = vectorizer\n","        self.similarity_cache = {}\n","\n","    def get_similar_candidates(self, candidate_idx, top_n=10, similarity_threshold=0.1):\n","        \"\"\"Get most similar candidates for a given candidate\"\"\"\n","        from sklearn.metrics.pairwise import cosine_similarity\n","\n","        # Get the TF-IDF vector for the candidate\n","        candidate_vector = self.df_tfidf.iloc[candidate_idx:candidate_idx+1]\n","\n","        # Compute similarity with all candidates\n","        similarities = cosine_similarity(candidate_vector, self.df_tfidf)[0]\n","\n","        # Get indices of most similar candidates (excluding self)\n","        similar_indices = np.argsort(similarities)[::-1][1:top_n+1]  # Exclude self (index 0)\n","        similar_scores = similarities[similar_indices]\n","\n","        # Filter by threshold\n","        valid_mask = similar_scores >= similarity_threshold\n","        similar_indices = similar_indices[valid_mask]\n","        similar_scores = similar_scores[valid_mask]\n","\n","        # Create results\n","        results = []\n","        for idx, score in zip(similar_indices, similar_scores):\n","            candidate_info = {\n","                'index': int(idx),\n","                'similarity_score': float(score),\n","                'nivel_profissional': self.df_application.iloc[idx].get('nivel_profissional', 'N/A'),\n","                'area_atuacao': self.df_application.iloc[idx].get('area_atuacao', 'N/A'),\n","                'nivel_academico': self.df_application.iloc[idx].get('nivel_academico', 'N/A'),\n","                'conhecimentos_preview': str(self.df_application.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n","            }\n","            results.append(candidate_info)\n","\n","        return results\n","\n","    def recommend_for_job_description(self, job_description, top_n=10):\n","        \"\"\"Find candidates similar to a job description\"\"\"\n","        from sklearn.metrics.pairwise import cosine_similarity\n","\n","        # Vectorize the job description using the same vectorizer\n","        job_vector = self.vectorizer.transform([job_description])\n","\n","        # Compute similarity with all candidates\n","        similarities = cosine_similarity(job_vector, self.df_tfidf)[0]\n","\n","        # Get top candidates\n","        top_indices = np.argsort(similarities)[::-1][:top_n]\n","        top_scores = similarities[top_indices]\n","\n","        # Create results\n","        results = []\n","        for idx, score in zip(top_indices, top_scores):\n","            candidate_info = {\n","                'index': int(idx),\n","                'match_score': float(score),\n","                'nivel_profissional': self.df_application.iloc[idx].get('nivel_profissional', 'N/A'),\n","                'area_atuacao': self.df_application.iloc[idx].get('area_atuacao', 'N/A'),\n","                'nivel_academico': self.df_application.iloc[idx].get('nivel_academico', 'N/A'),\n","                'conhecimentos_preview': str(self.df_application.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n","            }\n","            results.append(candidate_info)\n","\n","        return results\n","\n","# Initialize the recommendation system\n","print(\"Initializing Talent Recommendation System...\")\n","talent_recommender = TalentRecommendationSystem(\n","    df_tfidf_combined,\n","    df,\n","    vectorizer\n",")\n","\n","print(\"✅ Recommendation system ready!\")\n","print(f\"Loaded {len(df_tfidf_combined)} candidate profiles\")\n","print(f\"Vocabulary size: {len(vectorizer.vocabulary_)} features\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybK-668CWoP9","executionInfo":{"status":"ok","timestamp":1758982378475,"user_tz":180,"elapsed":929,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"}},"outputId":"cdac7743-f653-400f-9be4-f1ca3f9d62a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","TESTING TALENT RECOMMENDATION SYSTEM\n","============================================================\n","\n","🔍 TEST 1: Find Similar Candidates\n","\n","Top 5 candidates similar to candidate #50:\n","\n","1. Similarity: 0.488\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: revelo casado, 25 anos - brasileiro carteira de habilitação: ab formação acadêmica - bacharel em sis...\n","\n","2. Similarity: 0.376\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: leandro macris alves de souza revelo dados pessoais brasileiro – 29/09/86 - cnh a/b. jd. capuava – n...\n","\n","3. Similarity: 0.376\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: fullstack developer revelo resumo trabalhando com desenvolvimento desde 2016, com experiência no des...\n","\n","4. Similarity: 0.371\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: revelo francisco beltrão – pr resumo profissional profissional formado em sistemas de informação e c...\n","\n","5. Similarity: 0.371\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: revelo analista de testes perfil profissional sou analista de testes com três anos de experiência. s...\n","\n","============================================================\n","🎯 TEST 2: Job Matching\n","\n","Top 5 candidates for the job description:\n","\n","1. Match Score: 0.296\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: desenvolvedor web / mobile. conhecimento nas linguagens python e javascript, bem como alguns frameworks como: django, flask, react, vue e react native...\n","\n","2. Match Score: 0.261\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: brasileiro, solteiro, 26 anos dr alvarim vieira rios, 150 pouso alegre – mg objetivos manipulação e análise de dados utilizando algoritmos para tomada...\n","\n","3. Match Score: 0.186\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: desenvolvedor pl/sql.......\n","\n","4. Match Score: 0.179\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: habilidades: ruby,c++,c.c#,ios,android,javascript scala swift go css html scrum photoshop linux objective-c django angular.js node.js ember.js zend do...\n","\n","5. Match Score: 0.174\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: alexandre prieto profissional dinâmico, criativo e com espírito de equipe agilidade para se adequar aos padrões da empresa dominio avançado das lingua...\n","\n","============================================================\n","📊 SYSTEM PERFORMANCE METRICS\n","============================================================\n","Total candidates indexed: 1,000\n","Feature dimensions: 7,578\n","Memory usage (TF-IDF matrix): ~57.8 MB\n","Vocabulary size: 7,578 unique terms\n","\n","💾 Saving recommendation system components...\n","✅ Vectorizer saved\n","✅ Candidate mapping saved\n","\n","🎉 Talent Recommendation System Successfully Implemented!\n","Key improvements over the original approach:\n","✅ Consistent vocabulary across all batches\n","✅ Memory-efficient batch processing\n","✅ Scalable similarity computation\n","✅ Fast candidate matching and job description matching\n","✅ Reusable system components saved\n"]}],"source":["# Step 6: Test the Recommendation System\n","\n","print(\"=\"*60)\n","print(\"TESTING TALENT RECOMMENDATION SYSTEM\")\n","print(\"=\"*60)\n","\n","# Test 1: Find similar candidates to a specific candidate\n","print(\"\\n🔍 TEST 1: Find Similar Candidates\")\n","test_candidate_idx = 50  # Example candidate\n","similar_candidates = talent_recommender.get_similar_candidates(\n","    test_candidate_idx,\n","    top_n=5,\n","    similarity_threshold=0.1\n",")\n","\n","print(f\"\\nTop 5 candidates similar to candidate #{test_candidate_idx}:\")\n","for i, candidate in enumerate(similar_candidates, 1):\n","    print(f\"\\n{i}. Similarity: {candidate['similarity_score']:.3f}\")\n","    print(f\"   Level: {candidate['nivel_profissional']}\")\n","    print(f\"   Area: {candidate['area_atuacao']}\")\n","    print(f\"   Education: {candidate['nivel_academico']}\")\n","    print(f\"   Preview: {candidate['conhecimentos_preview'][:100]}...\")\n","\n","# Test 2: Find candidates for a job description\n","print(f\"\\n{'='*60}\")\n","print(\"🎯 TEST 2: Job Matching\")\n","job_description = \"\"\"\n","Procuramos um desenvolvedor Python sênior com experiência em:\n","- Desenvolvimento web com Django ou Flask\n","- Bancos de dados PostgreSQL e MongoDB\n","- APIs REST e microserviços\n","- Docker e Kubernetes\n","- Machine Learning com scikit-learn\n","- Experiência com AWS ou Azure\n","\"\"\"\n","\n","matching_candidates = talent_recommender.recommend_for_job_description(\n","    job_description,\n","    top_n=5\n",")\n","\n","print(f\"\\nTop 5 candidates for the job description:\")\n","for i, candidate in enumerate(matching_candidates, 1):\n","    print(f\"\\n{i}. Match Score: {candidate['match_score']:.3f}\")\n","    print(f\"   Level: {candidate['nivel_profissional']}\")\n","    print(f\"   Area: {candidate['area_atuacao']}\")\n","    print(f\"   Education: {candidate['nivel_academico']}\")\n","    print(f\"   Preview: {candidate['conhecimentos_preview'][:150]}...\")\n","\n","# Performance metrics\n","print(f\"\\n{'='*60}\")\n","print(\"📊 SYSTEM PERFORMANCE METRICS\")\n","print(\"=\"*60)\n","print(f\"Total candidates indexed: {len(df_tfidf_combined):,}\")\n","print(f\"Feature dimensions: {df_tfidf_combined.shape[1]:,}\")\n","print(f\"Memory usage (TF-IDF matrix): ~{df_tfidf_combined.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n","print(f\"Vocabulary size: {len(vectorizer.vocabulary_):,} unique terms\")\n","\n","# Save the system for future use\n","print(f\"\\n💾 Saving recommendation system components...\")\n","import joblib\n","\n","# Save vectorizer\n","joblib.dump(vectorizer, '/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_vectorizer.pkl')\n","print(\"✅ Vectorizer saved\")\n","\n","# Save candidate mapping\n","candidate_mapping = {\n","    'indices': df_tfidf_combined.index.tolist(),\n","    'total_candidates': len(df_tfidf_combined)\n","}\n","import json\n","with open('/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/candidate_mapping.json', 'w') as f:\n","    json.dump(candidate_mapping, f)\n","print(\"✅ Candidate mapping saved\")\n","\n","print(f\"\\n🎉 Talent Recommendation System Successfully Implemented!\")\n","print(\"Key improvements over the original approach:\")\n","print(\"✅ Consistent vocabulary across all batches\")\n","print(\"✅ Memory-efficient batch processing\")\n","print(\"✅ Scalable similarity computation\")\n","print(\"✅ Fast candidate matching and job description matching\")\n","print(\"✅ Reusable system components saved\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv (3.12.3)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}