{"cells":[{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4447,"status":"ok","timestamp":1759193988473,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"RiqvGiFzF8sY","outputId":"b9c9a436-e40a-4c83-b165-963476807a01"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package rslp to /root/nltk_data...\n","[nltk_data]   Package rslp is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('pt_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import re\n","import nltk\n","nltk.download(\"stopwords\")\n","nltk.download(\"punkt\")\n","nltk.download(\"punkt_tab\")\n","nltk.download('rslp')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import random\n","import time\n","import string\n","import unicodedata\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn import svm\n","from sklearn import metrics\n","import multiprocessing\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import linear_kernel\n","import glob\n","import spacy.cli\n","spacy.cli.download(\"pt_core_news_sm\")\n","import spacy\n","nlp = spacy.load(\"pt_core_news_sm\")\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import RSLPStemmer\n","stemmer = RSLPStemmer()\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZJxi1mELXasP","executionInfo":{"status":"ok","timestamp":1759201479167,"user_tz":180,"elapsed":944,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"}},"outputId":"7a0ac621-9691-492f-aea3-b44ba6cb2d6f"},"execution_count":138,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"-zjvi5nDq2tn"},"source":["# 1. Carrega base consolidada"]},{"cell_type":"code","execution_count":139,"metadata":{"executionInfo":{"elapsed":1390,"status":"ok","timestamp":1759201517074,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"FQRb3J0Vz-U0"},"outputs":[],"source":["df_path = '/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/Dados/silver/dados_processed/df_join_prospect_base.parquet'\n","df = pd.read_parquet(df_path)\n","df = df.head(100)"]},{"cell_type":"markdown","metadata":{"id":"bAoQicT0rqU-"},"source":["# 2. Métodos"]},{"cell_type":"code","execution_count":142,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1759201548722,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"PTxj_M1S20BL"},"outputs":[],"source":["def remove_person_names(text: str) -> str:\n","    doc = nlp(text)\n","    return \" \".join([token.text for token in doc if token.ent_type_ != \"PER\"])\n","\n","def normalize_accents(text: str) -> str:\n","    return unicodedata.normalize(\"NFKD\", text).encode(\"ASCII\", \"ignore\").decode(\"utf-8\")\n","\n","def remove_punctuation(text: str) -> str:\n","    table = str.maketrans({key: \" \" for key in string.punctuation})\n","    return text.translate(table)\n","\n","def normalize_str(text: str) -> str:\n","    text = text.lower()\n","    text = re.sub(r\"\\d+\", \" \", text)           # remove números\n","    text = remove_punctuation(text)            # remove pontuação\n","    text = normalize_accents(text)             # remove acentos\n","    text = re.sub(r\"\\s+\", \" \", text).strip()   # normaliza espaços\n","    return text\n","\n","def tokenizer(text: str):\n","    stop_words_br = set(nltk.corpus.stopwords.words(\"portuguese\"))\n","    if isinstance(text, str):\n","        text = normalize_str(text)                                              # normaliza string\n","        text = remove_person_names(text)                                        # remove nomes\n","        tokens = word_tokenize(text, language=\"portuguese\")                     # tokeniza para a lingua portuguesa\n","        tokens = [t for t in tokens if t not in stop_words_br and len(t) > 2]\n","        tokens = [stemmer.stem(t) for t in tokens]                              # stemiza tokens\n","        return tokens\n","    return None\n","\n","def tokenize_and_vectorize_fixed(df, campo_vetor, fitted_vectorizer, filename_prefix, batch_idx):\n","    \"\"\"\n","    Transforma um lote de dados usando um vetorizador TF-IDF já treinado.\n","    Salva o lote como arquivo Parquet.\n","    \"\"\"\n","    # Transformar o texto em matriz TF-IDF usando vocabulário existente\n","    vector_matrix = fitted_vectorizer.transform(df[campo_vetor].fillna(\"\"))\n","\n","    # Criar DataFrame com os vetores\n","    df_tfidf = pd.DataFrame(\n","        vector_matrix.toarray(),\n","        columns=fitted_vectorizer.get_feature_names_out(),\n","        index=df.index\n","    )\n","\n","    # Salvar batch\n","    output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n","    df_tfidf.to_parquet(output_file)\n","    print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n","\n","    return df_tfidf\n","\n","def combine_vector_batches(batch_files, output_file):\n","    \"\"\"\n","    Combina todos os arquivos de lote em um único DataFrame e salva como Parquet.\n","    \"\"\"\n","    print(\"Combinando todos os lotes...\")\n","    combined_dfs = []\n","\n","    for i, file in enumerate(batch_files):\n","        df_batch = pd.read_parquet(file)  # ✅ Lê o arquivo Parquet\n","        combined_dfs.append(df_batch)\n","        print(f\"Lote {i} carregado: {df_batch.shape}\")\n","\n","    # Combinar todos os DataFrames\n","    df_combined = pd.concat(combined_dfs, ignore_index=True)  # ignore_index=True reinicia os índices\n","    df_combined.to_parquet(output_file)\n","    print(f\"Conjunto combinado salvo: {df_combined.shape} -> {output_file}\")\n","\n","    return df_combined\n","\n","def combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=1000, output_dir=\"output\"):\n","    \"\"\"\n","    Treina o TF-IDF no dataset inteiro, processa em batches e combina todos os batches.\n","    \"\"\"\n","    # Treina o vetorizador em todos os dados\n","    print(\"Treinando o vetorizador em todo o conjunto de dados...\")\n","    vectorizer.fit(df[campo_vetor].fillna(\"\"))\n","\n","    print(f\"Tamanho do vocabulário: {len(vectorizer.vocabulary_)}\")\n","    print(\"Exemplo de features:\", list(vectorizer.get_feature_names_out())[:10])\n","\n","    # Processa em batches\n","    print(\"Processando lotes com vocabulário consistente...\")\n","    filename_prefix = f\"/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/application_processed_{campo_vetor}\"\n","    batch_files = []\n","\n","    for i in range(0, len(df), batch_size):\n","        batch_df = df.iloc[i:i+batch_size]\n","        batch_idx = i // batch_size\n","\n","        # Vetoriza batch\n","        #tokenize_and_vectorize_fixed(batch_df, campo_vetor, vectorizer, filename_prefix, batch_idx)\n","        X_tfidf, df_tfidf = vetoriza_input(df, campo_vetor, vectorizer)\n","\n","        # Salvar batch\n","        output_file = f\"{filename_prefix}_batch_{batch_idx}.parquet\"\n","        df_tfidf.to_parquet(output_file)\n","        print(f\"Lote {batch_idx} salvo com formato {df_tfidf.shape} em {output_file}\")\n","\n","        batch_files.append(f\"{filename_prefix}_batch_{batch_idx}.parquet\")\n","\n","        print(f\"Lote {batch_idx} concluído (linhas {i} até {min(i+batch_size, len(df))})\")\n","\n","    print(f\"\\n{len(batch_files)} lotes processados com sucesso!\")\n","    print(f\"Todos os lotes agora possuem as mesmas {len(vectorizer.vocabulary_)} features\")\n","\n","    # Combina todos os batches\n","    combined_output_file = f\"/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_pool_vectors_combined_{campo_vetor}.parquet\"\n","    df_tfidf_combined = combine_vector_batches(batch_files, combined_output_file)\n","\n","    return df_tfidf_combined, vectorizer, batch_files\n","\n","def vetoriza_input(df_input, campo_vetor, vectorizer):\n","    # Fit the vectorizer to the text data\n","    vectorizer.fit(df_input[campo_vetor].fillna(\"\"))\n","\n","    # Transform the text into TF-IDF vectors\n","    X_tfidf = vectorizer.transform(df_input[campo_vetor].fillna(\"\"))\n","\n","    # Insere vetores numa coluna única num dataframe\n","    df_tfidf = df_input.copy()\n","    df_tfidf['vetor_cv'] = [arr for arr in X_tfidf.toarray()]\n","\n","    return X_tfidf, df_tfidf\n","\n","def compute_similarity_batched(df_tfidf, campo_vetor, batch_size_sim=500, output_prefix='similarity_batch'):\n","    \"\"\"Calcular similaridade do cosseno em lotes para lidar com grandes conjuntos de dados\"\"\"\n","    from sklearn.metrics.pairwise import cosine_similarity\n","    import numpy as np\n","\n","    n_samples = len(df_tfidf)\n","    print(f\"Calculando similaridade para {n_samples} amostras em lotes de {batch_size_sim}\")\n","\n","    # Criar matriz de similaridade em lotes para gerenciar memória\n","    similarity_files = []\n","\n","    for i in range(0, n_samples, batch_size_sim):\n","        batch_end = min(i + batch_size_sim, n_samples)\n","        batch_data = df_tfidf.iloc[i:batch_end]\n","\n","        # Calcular similaridade entre este lote e TODOS os dados\n","        batch_similarity = cosine_similarity(batch_data, df_tfidf)\n","\n","        # Salvar similaridade do lote\n","        batch_file = f'/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/{output_prefix}_{i}_{batch_end}_{campo_vetor}.npy'\n","        np.save(batch_file, batch_similarity)\n","        similarity_files.append(batch_file)\n","\n","        print(f\"Similaridade do lote {i}-{batch_end} calculada: {batch_similarity.shape}\")\n","\n","    return similarity_files"]},{"cell_type":"markdown","source":["# 3. Tratamento para input inicial de dados do Streamlit"],"metadata":{"id":"a_1Axm5_sgMb"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","# Inputs de currículos\n","cv_input = [\n","    'Desenvolvedor python senior são paulo',\n","    'Desenvolvedor pandas senior rio de janeiro',\n","    'Python junior programador são paulo'\n","]\n","\n","# Dataframe com os inputs do streamleat\n","df_input = pd.DataFrame({'cv_candidato': cv_input})\n","\n","campo_vetor = 'cv_candidato'\n","\n","# Define TF-IDF vectorizer\n","vectorizer = TfidfVectorizer(\n","    max_features=10000,  # no limit on number of features\n","    min_df=1,           # include terms appearing in at least 1 document\n","    max_df=1.0          # include terms appearing in all documents\n",")\n","\n","X_tfidf, df_tfidf = vetoriza_input(df_input, campo_vetor)\n","\n","cosine_sim_matrix = cosine_similarity(X_tfidf)"],"metadata":{"id":"UrK7WIcfsj0y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Vetorização e cálculo da similaridade"],"metadata":{"id":"b9dyyPn2lh7k"}},{"cell_type":"code","execution_count":152,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2346,"status":"ok","timestamp":1759202426467,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"},"user_tz":180},"id":"T4T-SRgu7L_n","outputId":"bc0b22d9-d97f-475f-cbc9-5c4aa496f806"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vetorizando campo: cv_pt\n","Criando vocabulário a partir de todos os dados...\n","Treinando o vetorizador em todo o conjunto de dados...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Tamanho do vocabulário: 342\n","Exemplo de features: ['aasp', 'abril', 'academ', 'acompanh', 'acopl', 'administr', 'ado', 'advent', 'advog', 'agenc']\n","Processando lotes com vocabulário consistente...\n","Lote 0 salvo com formato (100, 23) em /content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/application_processed_cv_pt_batch_0.parquet\n","Lote 0 concluído (linhas 0 até 100)\n","\n","1 lotes processados com sucesso!\n","Todos os lotes agora possuem as mesmas 342 features\n","Combinando todos os lotes...\n","Lote 0 carregado: (100, 23)\n","Conjunto combinado salvo: (100, 23) -> /content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_pool_vectors_combined_cv_pt.parquet\n"]}],"source":["lista_campos_vetor = ['cv_pt'] # trocar para os campos a serem utilizados\n","\n","for campo_vetor in lista_campos_vetor:\n","  print(f\"Vetorizando campo: {campo_vetor}\")\n","  print(\"Criando vocabulário a partir de todos os dados...\")\n","\n","  # Ajusta o vetorizador em TODOS os dados para criar vocabulário consistente\n","  vectorizer = TfidfVectorizer(\n","      tokenizer=tokenizer,\n","      max_features=10000,\n","      min_df=2,\n","      max_df=0.8\n","  )\n","\n","  # Processa e combina batches\n","  df_tfidf_combined, vectorizer, batch_files = combine_tfidf_batches(df, campo_vetor, vectorizer, batch_size=100, output_dir=\"output\")\n","\n","  # Salva em parquet\n","  df_tfidf_combined.to_parquet('/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/Dados/gold/df_join_tfidf.parquet')"]},{"cell_type":"markdown","source":["# 4. Sistema de Recomendação"],"metadata":{"id":"_0KOOzr6qV5k"}},{"cell_type":"code","execution_count":158,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bBq4BS4-WoP8","executionInfo":{"status":"ok","timestamp":1759203601465,"user_tz":180,"elapsed":7,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"}},"outputId":"493eae88-6a04-4baf-9170-d596b657202b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing Talent Recommendation System...\n","✅ Recommendation system ready!\n","Loaded 100 candidate profiles\n","Vocabulary size: 342 features\n"]}],"source":["# Step 5: Efficient Recommendation System\n","class TalentRecommendationSystem:\n","    def __init__(self, df_tfidf, df_application_original, vectorizer):\n","        self.df_tfidf = df_tfidf\n","        self.df_application = df_application_original\n","        self.vectorizer = vectorizer\n","        self.similarity_cache = {}\n","\n","    def get_similar_candidates(self, candidate_idx, top_n=10, similarity_threshold=0.1):\n","        \"\"\"Get most similar candidates for a given candidate\"\"\"\n","        from sklearn.metrics.pairwise import cosine_similarity\n","\n","        # Get the TF-IDF vector for the candidate\n","        candidate_vector = self.df_tfidf.iloc[candidate_idx:candidate_idx+1]\n","\n","        # Compute similarity with all candidates\n","        similarities = cosine_similarity(candidate_vector, self.df_tfidf)[0]\n","\n","        # Get indices of most similar candidates (excluding self)\n","        similar_indices = np.argsort(similarities)[::-1][1:top_n+1]  # Exclude self (index 0)\n","        similar_scores = similarities[similar_indices]\n","\n","        # Filter by threshold\n","        valid_mask = similar_scores >= similarity_threshold\n","        similar_indices = similar_indices[valid_mask]\n","        similar_scores = similar_scores[valid_mask]\n","\n","        # Create results\n","        results = []\n","        for idx, score in zip(similar_indices, similar_scores):\n","            candidate_info = {\n","                'index': int(idx),\n","                'similarity_score': float(score),\n","                'nivel_profissional': self.df_application.iloc[idx].get('nivel_profissional', 'N/A'),\n","                'area_atuacao': self.df_application.iloc[idx].get('area_atuacao', 'N/A'),\n","                'nivel_academico': self.df_application.iloc[idx].get('nivel_academico', 'N/A'),\n","                'conhecimentos_preview': str(self.df_application.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n","            }\n","            results.append(candidate_info)\n","\n","        return results\n","\n","    def recommend_for_job_description(self, job_description, top_n=10):\n","        \"\"\"Find candidates similar to a job description\"\"\"\n","        from sklearn.metrics.pairwise import cosine_similarity\n","\n","        # Vectorize the job description using the same vectorizer\n","        job_vector = self.vectorizer.transform([job_description])\n","\n","        # Compute similarity with all candidates\n","        similarities = cosine_similarity(job_vector, self.df_tfidf)[0]\n","\n","        # Get top candidates\n","        top_indices = np.argsort(similarities)[::-1][:top_n]\n","        top_scores = similarities[top_indices]\n","\n","        # Create results\n","        results = []\n","        for idx, score in zip(top_indices, top_scores):\n","            candidate_info = {\n","                'index': int(idx),\n","                'match_score': float(score),\n","                'nivel_profissional': self.df_application.iloc[idx].get('nivel_profissional', 'N/A'),\n","                'area_atuacao': self.df_application.iloc[idx].get('area_atuacao', 'N/A'),\n","                'nivel_academico': self.df_application.iloc[idx].get('nivel_academico', 'N/A'),\n","                'conhecimentos_preview': str(self.df_application.iloc[idx].get(campo_vetor, ''))[:200] + '...'\n","            }\n","            results.append(candidate_info)\n","\n","        return results\n","\n","# Initialize the recommendation system\n","print(\"Initializing Talent Recommendation System...\")\n","talent_recommender = TalentRecommendationSystem(\n","    df_tfidf_combined,\n","    df,\n","    vectorizer\n",")\n","\n","print(\"✅ Recommendation system ready!\")\n","print(f\"Loaded {len(df_tfidf_combined)} candidate profiles\")\n","print(f\"Vocabulary size: {len(vectorizer.vocabulary_)} features\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybK-668CWoP9","executionInfo":{"status":"ok","timestamp":1758982378475,"user_tz":180,"elapsed":929,"user":{"displayName":"Lucas Ramos","userId":"09216526235417022731"}},"outputId":"cdac7743-f653-400f-9be4-f1ca3f9d62a2"},"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","TESTING TALENT RECOMMENDATION SYSTEM\n","============================================================\n","\n","🔍 TEST 1: Find Similar Candidates\n","\n","Top 5 candidates similar to candidate #50:\n","\n","1. Similarity: 0.488\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: revelo casado, 25 anos - brasileiro carteira de habilitação: ab formação acadêmica - bacharel em sis...\n","\n","2. Similarity: 0.376\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: leandro macris alves de souza revelo dados pessoais brasileiro – 29/09/86 - cnh a/b. jd. capuava – n...\n","\n","3. Similarity: 0.376\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: fullstack developer revelo resumo trabalhando com desenvolvimento desde 2016, com experiência no des...\n","\n","4. Similarity: 0.371\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: revelo francisco beltrão – pr resumo profissional profissional formado em sistemas de informação e c...\n","\n","5. Similarity: 0.371\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: revelo analista de testes perfil profissional sou analista de testes com três anos de experiência. s...\n","\n","============================================================\n","🎯 TEST 2: Job Matching\n","\n","Top 5 candidates for the job description:\n","\n","1. Match Score: 0.296\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: desenvolvedor web / mobile. conhecimento nas linguagens python e javascript, bem como alguns frameworks como: django, flask, react, vue e react native...\n","\n","2. Match Score: 0.261\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: brasileiro, solteiro, 26 anos dr alvarim vieira rios, 150 pouso alegre – mg objetivos manipulação e análise de dados utilizando algoritmos para tomada...\n","\n","3. Match Score: 0.186\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: desenvolvedor pl/sql.......\n","\n","4. Match Score: 0.179\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: habilidades: ruby,c++,c.c#,ios,android,javascript scala swift go css html scrum photoshop linux objective-c django angular.js node.js ember.js zend do...\n","\n","5. Match Score: 0.174\n","   Level: \n","   Area: \n","   Education: None\n","   Preview: alexandre prieto profissional dinâmico, criativo e com espírito de equipe agilidade para se adequar aos padrões da empresa dominio avançado das lingua...\n","\n","============================================================\n","📊 SYSTEM PERFORMANCE METRICS\n","============================================================\n","Total candidates indexed: 1,000\n","Feature dimensions: 7,578\n","Memory usage (TF-IDF matrix): ~57.8 MB\n","Vocabulary size: 7,578 unique terms\n","\n","💾 Saving recommendation system components...\n","✅ Vectorizer saved\n","✅ Candidate mapping saved\n","\n","🎉 Talent Recommendation System Successfully Implemented!\n","Key improvements over the original approach:\n","✅ Consistent vocabulary across all batches\n","✅ Memory-efficient batch processing\n","✅ Scalable similarity computation\n","✅ Fast candidate matching and job description matching\n","✅ Reusable system components saved\n"]}],"source":["# Step 6: Test the Recommendation System\n","\n","print(\"=\"*60)\n","print(\"TESTING TALENT RECOMMENDATION SYSTEM\")\n","print(\"=\"*60)\n","\n","# Test 1: Find similar candidates to a specific candidate\n","print(\"\\n🔍 TEST 1: Find Similar Candidates\")\n","test_candidate_idx = 50  # Example candidate\n","similar_candidates = talent_recommender.get_similar_candidates(\n","    test_candidate_idx,\n","    top_n=5,\n","    similarity_threshold=0.1\n",")\n","\n","print(f\"\\nTop 5 candidates similar to candidate #{test_candidate_idx}:\")\n","for i, candidate in enumerate(similar_candidates, 1):\n","    print(f\"\\n{i}. Similarity: {candidate['similarity_score']:.3f}\")\n","    print(f\"   Level: {candidate['nivel_profissional']}\")\n","    print(f\"   Area: {candidate['area_atuacao']}\")\n","    print(f\"   Education: {candidate['nivel_academico']}\")\n","    print(f\"   Preview: {candidate['conhecimentos_preview'][:100]}...\")\n","\n","# Test 2: Find candidates for a job description\n","print(f\"\\n{'='*60}\")\n","print(\"🎯 TEST 2: Job Matching\")\n","job_description = \"\"\"\n","Procuramos um desenvolvedor Python sênior com experiência em:\n","- Desenvolvimento web com Django ou Flask\n","- Bancos de dados PostgreSQL e MongoDB\n","- APIs REST e microserviços\n","- Docker e Kubernetes\n","- Machine Learning com scikit-learn\n","- Experiência com AWS ou Azure\n","\"\"\"\n","\n","matching_candidates = talent_recommender.recommend_for_job_description(\n","    job_description,\n","    top_n=5\n",")\n","\n","print(f\"\\nTop 5 candidates for the job description:\")\n","for i, candidate in enumerate(matching_candidates, 1):\n","    print(f\"\\n{i}. Match Score: {candidate['match_score']:.3f}\")\n","    print(f\"   Level: {candidate['nivel_profissional']}\")\n","    print(f\"   Area: {candidate['area_atuacao']}\")\n","    print(f\"   Education: {candidate['nivel_academico']}\")\n","    print(f\"   Preview: {candidate['conhecimentos_preview'][:150]}...\")\n","\n","# Performance metrics\n","print(f\"\\n{'='*60}\")\n","print(\"📊 SYSTEM PERFORMANCE METRICS\")\n","print(\"=\"*60)\n","print(f\"Total candidates indexed: {len(df_tfidf_combined):,}\")\n","print(f\"Feature dimensions: {df_tfidf_combined.shape[1]:,}\")\n","print(f\"Memory usage (TF-IDF matrix): ~{df_tfidf_combined.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB\")\n","print(f\"Vocabulary size: {len(vectorizer.vocabulary_):,} unique terms\")\n","\n","# Save the system for future use\n","print(f\"\\n💾 Saving recommendation system components...\")\n","import joblib\n","\n","# Save vectorizer\n","joblib.dump(vectorizer, '/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/talent_vectorizer.pkl')\n","print(\"✅ Vectorizer saved\")\n","\n","# Save candidate mapping\n","candidate_mapping = {\n","    'indices': df_tfidf_combined.index.tolist(),\n","    'total_candidates': len(df_tfidf_combined)\n","}\n","import json\n","with open('/content/drive/MyDrive/Pós Tech/Tech Challenges/Tech Challenge 5/notebooks/ramos/candidate_mapping.json', 'w') as f:\n","    json.dump(candidate_mapping, f)\n","print(\"✅ Candidate mapping saved\")\n","\n","print(f\"\\n🎉 Talent Recommendation System Successfully Implemented!\")\n","print(\"Key improvements over the original approach:\")\n","print(\"✅ Consistent vocabulary across all batches\")\n","print(\"✅ Memory-efficient batch processing\")\n","print(\"✅ Scalable similarity computation\")\n","print(\"✅ Fast candidate matching and job description matching\")\n","print(\"✅ Reusable system components saved\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv (3.12.3)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}